{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eeg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnirudhDesai777/legendary-enigma/blob/master/eeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX-gL_eWvUYS",
        "outputId": "b5e229e7-b5e8-46e0-c2ff-01cba59ba9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Intalling Pyeeg\n",
        "!pip install git+https://github.com/forrestbao/pyeeg.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/forrestbao/pyeeg.git\n",
            "  Cloning https://github.com/forrestbao/pyeeg.git to /tmp/pip-req-build-jbg_2xu3\n",
            "  Running command git clone -q https://github.com/forrestbao/pyeeg.git /tmp/pip-req-build-jbg_2xu3\n",
            "Requirement already satisfied (use --upgrade to upgrade): pyeeg==0.4.4 from git+https://github.com/forrestbao/pyeeg.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyeeg==0.4.4) (1.18.5)\n",
            "Building wheels for collected packages: pyeeg\n",
            "  Building wheel for pyeeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyeeg: filename=pyeeg-0.4.4-py2.py3-none-any.whl size=28122 sha256=df73fd069ac763c478e476810ffa9d2de5b848b6913c8511b7e56e07972948d4\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6nwiaf72/wheels/2d/3f/ad/106d4fc80b61d1ea1fc18e76e7439fd98aa043d83d58eae741\n",
            "Successfully built pyeeg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iwtd-G8hvbrB"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "import pickle as pickle\n",
        "import pandas as pd\n",
        "import pyeeg as pe\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM,BatchNormalization,Activation"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yo4ngkMmvsU8",
        "outputId": "f333be43-16e4-4d90-93b9-de77e4f49026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72i-9WAqgkjL"
      },
      "source": [
        "def getPSD(sub):\n",
        "    # channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
        "    channel = [2, 3, 4, 6, 7, 11, 15, 17, 20, 24, 25, 27, 28, 29]\n",
        "    band = [4,8,12,16,25,45] #5 bands\n",
        "    window_size = 256 #Averaging band power of 2 sec\n",
        "    step_size = 16 #Each 0.125 sec update once\n",
        "    sample_rate = 70 #Sampling rate of 128 Hz\n",
        "    temp = []\n",
        "    with open(r'/content/drive/My Drive/data_preprocessed_python/s'+sub+'.dat','rb') as file:\n",
        "        \n",
        "        subject = pickle.load(file, encoding='latin1')\n",
        "        \n",
        "        for i in range(1,40):   # trials\n",
        "            raw_data = subject['data'][i]\n",
        "            raw_labels = subject['labels'][i]\n",
        "            start = 0\n",
        "            while start + window_size < len (raw_data[i]):\n",
        "                temp_array = []\n",
        "                temp_data = []\n",
        "                for j in channel:\n",
        "                    X = raw_data[j][start: start + window_size]\n",
        "                    Y = pe.bin_power(X, band, sample_rate)\n",
        "                    temp_data = temp_data + list(Y[0])\n",
        "                temp_array.append(np.array(temp_data))\n",
        "                temp_array.append(raw_labels)\n",
        "                temp.append(np.array(temp_array))\n",
        "                start = start + step_size\n",
        "        temp = np.array(temp)\n",
        "        np.save('out\\s' + sub, temp, allow_pickle=True, fix_imports=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwySLO7Lg0Qx"
      },
      "source": [
        "subjectList=['01','02']\n",
        "# for i in range(0,32):\n",
        "#     if i < 10:\n",
        "#         subjectList.append('0' + str(i))\n",
        "#     else:\n",
        "#         subjectList.append(str(i))\n",
        "for sub in subjectList:\n",
        "    getPSD(sub)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGhbCnfFg2qd"
      },
      "source": [
        "data= []\n",
        "label = []\n",
        "for subjects in subjectList:\n",
        "  \n",
        "\n",
        "    with open('/content/out\\s' + subjects + '.npy', 'rb') as file:\n",
        "        sub = np.load(file,allow_pickle=True)\n",
        "        for i in range (0,sub.shape[0]):\n",
        "          data.append(sub[i][0])\n",
        "          label.append(sub[i][1])\n",
        "np.save('data', np.array(data), allow_pickle=True, fix_imports=True)\n",
        "np.save('label', np.array(label), allow_pickle=True, fix_imports=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQM-CNbIg-wD"
      },
      "source": [
        "df=pd.DataFrame(data=data)\n",
        "df.to_csv(\"data.csv\",index=False)\n",
        "\n",
        "df1=pd.DataFrame(data=label)\n",
        "df1.to_csv(\"label.csv\",index=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN3Py6vog_Xf",
        "outputId": "5db995be-fd96-410e-fdb5-911973999300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "data=pd.read_csv(\"data.csv\")\n",
        "data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2631.150479</td>\n",
              "      <td>1358.957902</td>\n",
              "      <td>982.274705</td>\n",
              "      <td>955.342880</td>\n",
              "      <td>294.874029</td>\n",
              "      <td>2551.385311</td>\n",
              "      <td>1454.875372</td>\n",
              "      <td>1160.444261</td>\n",
              "      <td>1868.547487</td>\n",
              "      <td>735.610538</td>\n",
              "      <td>2184.991704</td>\n",
              "      <td>1227.991908</td>\n",
              "      <td>637.224650</td>\n",
              "      <td>703.920851</td>\n",
              "      <td>446.074558</td>\n",
              "      <td>2540.103438</td>\n",
              "      <td>1130.444764</td>\n",
              "      <td>905.074784</td>\n",
              "      <td>663.709817</td>\n",
              "      <td>269.865088</td>\n",
              "      <td>2125.004953</td>\n",
              "      <td>1349.397472</td>\n",
              "      <td>1179.832701</td>\n",
              "      <td>1613.622506</td>\n",
              "      <td>124.784937</td>\n",
              "      <td>3017.045150</td>\n",
              "      <td>1133.591773</td>\n",
              "      <td>751.750974</td>\n",
              "      <td>1293.610237</td>\n",
              "      <td>246.359972</td>\n",
              "      <td>2942.386187</td>\n",
              "      <td>917.357900</td>\n",
              "      <td>528.091326</td>\n",
              "      <td>503.508233</td>\n",
              "      <td>244.957645</td>\n",
              "      <td>2500.020852</td>\n",
              "      <td>1010.887319</td>\n",
              "      <td>820.172336</td>\n",
              "      <td>899.212773</td>\n",
              "      <td>324.449719</td>\n",
              "      <td>2370.011435</td>\n",
              "      <td>1355.044789</td>\n",
              "      <td>811.638916</td>\n",
              "      <td>1035.489305</td>\n",
              "      <td>138.105326</td>\n",
              "      <td>1692.855714</td>\n",
              "      <td>1021.895232</td>\n",
              "      <td>562.791069</td>\n",
              "      <td>694.489695</td>\n",
              "      <td>81.809478</td>\n",
              "      <td>2658.606580</td>\n",
              "      <td>1241.722987</td>\n",
              "      <td>950.618845</td>\n",
              "      <td>820.843586</td>\n",
              "      <td>181.958041</td>\n",
              "      <td>2356.922525</td>\n",
              "      <td>1065.201648</td>\n",
              "      <td>559.659018</td>\n",
              "      <td>665.610362</td>\n",
              "      <td>139.443419</td>\n",
              "      <td>3316.131121</td>\n",
              "      <td>1239.898426</td>\n",
              "      <td>866.757418</td>\n",
              "      <td>1079.545210</td>\n",
              "      <td>178.715440</td>\n",
              "      <td>2995.306796</td>\n",
              "      <td>1132.885419</td>\n",
              "      <td>636.610161</td>\n",
              "      <td>851.187212</td>\n",
              "      <td>200.616797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2581.405819</td>\n",
              "      <td>1483.656365</td>\n",
              "      <td>868.999122</td>\n",
              "      <td>954.211467</td>\n",
              "      <td>207.157127</td>\n",
              "      <td>2712.013984</td>\n",
              "      <td>1167.513223</td>\n",
              "      <td>1127.247373</td>\n",
              "      <td>1790.977339</td>\n",
              "      <td>165.360472</td>\n",
              "      <td>2137.952410</td>\n",
              "      <td>1076.853752</td>\n",
              "      <td>449.022896</td>\n",
              "      <td>718.317714</td>\n",
              "      <td>109.872722</td>\n",
              "      <td>2324.871247</td>\n",
              "      <td>1144.706774</td>\n",
              "      <td>759.431074</td>\n",
              "      <td>693.737420</td>\n",
              "      <td>434.684496</td>\n",
              "      <td>2382.258738</td>\n",
              "      <td>1278.406931</td>\n",
              "      <td>1106.524307</td>\n",
              "      <td>1669.301133</td>\n",
              "      <td>78.347261</td>\n",
              "      <td>2864.148148</td>\n",
              "      <td>1202.543244</td>\n",
              "      <td>713.670122</td>\n",
              "      <td>1220.464349</td>\n",
              "      <td>171.134552</td>\n",
              "      <td>2684.602716</td>\n",
              "      <td>694.236847</td>\n",
              "      <td>452.038821</td>\n",
              "      <td>492.443596</td>\n",
              "      <td>60.167552</td>\n",
              "      <td>2272.257600</td>\n",
              "      <td>978.493430</td>\n",
              "      <td>849.494194</td>\n",
              "      <td>920.779985</td>\n",
              "      <td>258.220013</td>\n",
              "      <td>2257.514548</td>\n",
              "      <td>1290.011467</td>\n",
              "      <td>833.622924</td>\n",
              "      <td>956.613296</td>\n",
              "      <td>185.404789</td>\n",
              "      <td>1808.641468</td>\n",
              "      <td>1030.016079</td>\n",
              "      <td>580.209842</td>\n",
              "      <td>633.681823</td>\n",
              "      <td>23.591388</td>\n",
              "      <td>2564.086359</td>\n",
              "      <td>1266.424853</td>\n",
              "      <td>822.154165</td>\n",
              "      <td>795.378436</td>\n",
              "      <td>94.973731</td>\n",
              "      <td>2247.243240</td>\n",
              "      <td>944.242893</td>\n",
              "      <td>459.721437</td>\n",
              "      <td>636.732476</td>\n",
              "      <td>140.012426</td>\n",
              "      <td>3103.138653</td>\n",
              "      <td>1167.706063</td>\n",
              "      <td>824.685656</td>\n",
              "      <td>1026.452662</td>\n",
              "      <td>407.743916</td>\n",
              "      <td>2778.124424</td>\n",
              "      <td>1083.610349</td>\n",
              "      <td>562.980483</td>\n",
              "      <td>823.352860</td>\n",
              "      <td>94.360409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2558.295428</td>\n",
              "      <td>1610.831205</td>\n",
              "      <td>849.374201</td>\n",
              "      <td>930.752733</td>\n",
              "      <td>89.641656</td>\n",
              "      <td>2685.017537</td>\n",
              "      <td>1404.286630</td>\n",
              "      <td>1089.193772</td>\n",
              "      <td>1809.607435</td>\n",
              "      <td>140.086932</td>\n",
              "      <td>2137.573669</td>\n",
              "      <td>1145.901937</td>\n",
              "      <td>449.204238</td>\n",
              "      <td>719.870940</td>\n",
              "      <td>33.926975</td>\n",
              "      <td>2233.064223</td>\n",
              "      <td>1202.503548</td>\n",
              "      <td>777.563949</td>\n",
              "      <td>775.486863</td>\n",
              "      <td>904.494024</td>\n",
              "      <td>2353.730796</td>\n",
              "      <td>1264.779823</td>\n",
              "      <td>1230.317262</td>\n",
              "      <td>1582.081182</td>\n",
              "      <td>384.827387</td>\n",
              "      <td>2809.183146</td>\n",
              "      <td>1332.937492</td>\n",
              "      <td>725.159856</td>\n",
              "      <td>1198.954433</td>\n",
              "      <td>35.875445</td>\n",
              "      <td>2603.049393</td>\n",
              "      <td>784.931176</td>\n",
              "      <td>449.839612</td>\n",
              "      <td>572.106446</td>\n",
              "      <td>549.609360</td>\n",
              "      <td>2123.128448</td>\n",
              "      <td>994.149127</td>\n",
              "      <td>883.888678</td>\n",
              "      <td>933.947664</td>\n",
              "      <td>567.515704</td>\n",
              "      <td>2144.512682</td>\n",
              "      <td>1262.144109</td>\n",
              "      <td>849.144617</td>\n",
              "      <td>902.341927</td>\n",
              "      <td>489.022355</td>\n",
              "      <td>1717.127153</td>\n",
              "      <td>1050.533318</td>\n",
              "      <td>633.299146</td>\n",
              "      <td>655.749096</td>\n",
              "      <td>363.058518</td>\n",
              "      <td>2378.546453</td>\n",
              "      <td>1248.186272</td>\n",
              "      <td>926.515719</td>\n",
              "      <td>857.430873</td>\n",
              "      <td>557.419808</td>\n",
              "      <td>2209.077061</td>\n",
              "      <td>952.971620</td>\n",
              "      <td>459.738613</td>\n",
              "      <td>566.052414</td>\n",
              "      <td>103.880376</td>\n",
              "      <td>3069.033298</td>\n",
              "      <td>1166.310711</td>\n",
              "      <td>895.660584</td>\n",
              "      <td>1000.482320</td>\n",
              "      <td>387.489479</td>\n",
              "      <td>2523.804396</td>\n",
              "      <td>1059.181888</td>\n",
              "      <td>670.962054</td>\n",
              "      <td>793.328897</td>\n",
              "      <td>293.623898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2582.142347</td>\n",
              "      <td>1637.480330</td>\n",
              "      <td>927.589794</td>\n",
              "      <td>908.028093</td>\n",
              "      <td>15.189624</td>\n",
              "      <td>2827.300379</td>\n",
              "      <td>1382.212253</td>\n",
              "      <td>1114.313571</td>\n",
              "      <td>1763.913061</td>\n",
              "      <td>443.838430</td>\n",
              "      <td>2194.569664</td>\n",
              "      <td>1166.239323</td>\n",
              "      <td>434.730027</td>\n",
              "      <td>759.082749</td>\n",
              "      <td>345.898603</td>\n",
              "      <td>2053.273117</td>\n",
              "      <td>1062.547758</td>\n",
              "      <td>738.221718</td>\n",
              "      <td>702.557202</td>\n",
              "      <td>307.856869</td>\n",
              "      <td>2469.579010</td>\n",
              "      <td>1265.779549</td>\n",
              "      <td>1254.009664</td>\n",
              "      <td>1604.353916</td>\n",
              "      <td>69.202789</td>\n",
              "      <td>3397.880592</td>\n",
              "      <td>1429.708838</td>\n",
              "      <td>769.081868</td>\n",
              "      <td>1160.194091</td>\n",
              "      <td>21.013955</td>\n",
              "      <td>2777.223422</td>\n",
              "      <td>760.900793</td>\n",
              "      <td>478.261823</td>\n",
              "      <td>482.406019</td>\n",
              "      <td>104.076370</td>\n",
              "      <td>2255.719655</td>\n",
              "      <td>974.306655</td>\n",
              "      <td>835.678527</td>\n",
              "      <td>850.625649</td>\n",
              "      <td>496.629310</td>\n",
              "      <td>1973.626598</td>\n",
              "      <td>1341.330991</td>\n",
              "      <td>856.488587</td>\n",
              "      <td>950.808709</td>\n",
              "      <td>279.920595</td>\n",
              "      <td>1704.936262</td>\n",
              "      <td>1033.381810</td>\n",
              "      <td>617.749557</td>\n",
              "      <td>613.262895</td>\n",
              "      <td>100.443751</td>\n",
              "      <td>2208.557378</td>\n",
              "      <td>1215.232152</td>\n",
              "      <td>863.495946</td>\n",
              "      <td>829.153716</td>\n",
              "      <td>405.477393</td>\n",
              "      <td>2170.972788</td>\n",
              "      <td>944.812815</td>\n",
              "      <td>440.464652</td>\n",
              "      <td>531.830586</td>\n",
              "      <td>108.144776</td>\n",
              "      <td>3025.074576</td>\n",
              "      <td>1148.269068</td>\n",
              "      <td>875.540992</td>\n",
              "      <td>932.441207</td>\n",
              "      <td>30.044974</td>\n",
              "      <td>2442.494117</td>\n",
              "      <td>1027.526851</td>\n",
              "      <td>640.150561</td>\n",
              "      <td>803.652406</td>\n",
              "      <td>43.832560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2416.208172</td>\n",
              "      <td>1661.402028</td>\n",
              "      <td>887.095363</td>\n",
              "      <td>907.451324</td>\n",
              "      <td>200.345635</td>\n",
              "      <td>2859.881193</td>\n",
              "      <td>1329.604465</td>\n",
              "      <td>1097.509021</td>\n",
              "      <td>1689.867430</td>\n",
              "      <td>333.974464</td>\n",
              "      <td>2417.757202</td>\n",
              "      <td>1160.579091</td>\n",
              "      <td>528.398177</td>\n",
              "      <td>662.698281</td>\n",
              "      <td>283.931954</td>\n",
              "      <td>2187.138595</td>\n",
              "      <td>1109.866104</td>\n",
              "      <td>746.658845</td>\n",
              "      <td>661.946026</td>\n",
              "      <td>64.543555</td>\n",
              "      <td>2674.531848</td>\n",
              "      <td>1292.502586</td>\n",
              "      <td>1291.908072</td>\n",
              "      <td>1587.051168</td>\n",
              "      <td>586.294502</td>\n",
              "      <td>3425.566271</td>\n",
              "      <td>1510.852054</td>\n",
              "      <td>754.968074</td>\n",
              "      <td>1194.726720</td>\n",
              "      <td>272.584421</td>\n",
              "      <td>2768.836449</td>\n",
              "      <td>823.946899</td>\n",
              "      <td>473.824722</td>\n",
              "      <td>483.399688</td>\n",
              "      <td>39.856268</td>\n",
              "      <td>2171.955831</td>\n",
              "      <td>954.136812</td>\n",
              "      <td>850.446170</td>\n",
              "      <td>819.243168</td>\n",
              "      <td>260.948272</td>\n",
              "      <td>1852.724462</td>\n",
              "      <td>1329.715186</td>\n",
              "      <td>822.756196</td>\n",
              "      <td>916.225476</td>\n",
              "      <td>91.063499</td>\n",
              "      <td>1674.455280</td>\n",
              "      <td>1074.925170</td>\n",
              "      <td>611.528447</td>\n",
              "      <td>615.839764</td>\n",
              "      <td>153.417477</td>\n",
              "      <td>2202.963790</td>\n",
              "      <td>1275.565874</td>\n",
              "      <td>878.788279</td>\n",
              "      <td>816.327430</td>\n",
              "      <td>373.288374</td>\n",
              "      <td>2150.783333</td>\n",
              "      <td>917.908428</td>\n",
              "      <td>454.373054</td>\n",
              "      <td>517.379643</td>\n",
              "      <td>200.964120</td>\n",
              "      <td>2944.723495</td>\n",
              "      <td>1157.844530</td>\n",
              "      <td>876.652905</td>\n",
              "      <td>931.461369</td>\n",
              "      <td>336.005679</td>\n",
              "      <td>2371.751623</td>\n",
              "      <td>1087.871189</td>\n",
              "      <td>663.840495</td>\n",
              "      <td>876.780429</td>\n",
              "      <td>488.449395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38059</th>\n",
              "      <td>2422.256366</td>\n",
              "      <td>736.276173</td>\n",
              "      <td>739.160028</td>\n",
              "      <td>878.484221</td>\n",
              "      <td>205.638763</td>\n",
              "      <td>3913.822489</td>\n",
              "      <td>1712.742725</td>\n",
              "      <td>1250.070723</td>\n",
              "      <td>2663.207147</td>\n",
              "      <td>307.704952</td>\n",
              "      <td>3034.172384</td>\n",
              "      <td>1001.466768</td>\n",
              "      <td>922.444376</td>\n",
              "      <td>1777.169384</td>\n",
              "      <td>303.845027</td>\n",
              "      <td>2176.545755</td>\n",
              "      <td>764.340141</td>\n",
              "      <td>493.361712</td>\n",
              "      <td>836.021303</td>\n",
              "      <td>329.483480</td>\n",
              "      <td>7734.079842</td>\n",
              "      <td>2848.830012</td>\n",
              "      <td>3242.953289</td>\n",
              "      <td>4572.305213</td>\n",
              "      <td>414.484590</td>\n",
              "      <td>4242.368971</td>\n",
              "      <td>2030.341054</td>\n",
              "      <td>2098.271430</td>\n",
              "      <td>3816.592780</td>\n",
              "      <td>391.623601</td>\n",
              "      <td>1355.386930</td>\n",
              "      <td>592.356800</td>\n",
              "      <td>525.266944</td>\n",
              "      <td>710.530405</td>\n",
              "      <td>61.889597</td>\n",
              "      <td>2709.602345</td>\n",
              "      <td>1310.013669</td>\n",
              "      <td>1371.464884</td>\n",
              "      <td>2425.974327</td>\n",
              "      <td>356.911940</td>\n",
              "      <td>6465.505328</td>\n",
              "      <td>2217.550253</td>\n",
              "      <td>2014.013955</td>\n",
              "      <td>4056.024129</td>\n",
              "      <td>375.570282</td>\n",
              "      <td>2146.113905</td>\n",
              "      <td>690.627468</td>\n",
              "      <td>715.503508</td>\n",
              "      <td>1070.172371</td>\n",
              "      <td>197.823032</td>\n",
              "      <td>3655.929585</td>\n",
              "      <td>1218.426303</td>\n",
              "      <td>1104.701520</td>\n",
              "      <td>1590.369691</td>\n",
              "      <td>99.028916</td>\n",
              "      <td>2585.498425</td>\n",
              "      <td>917.938754</td>\n",
              "      <td>749.145698</td>\n",
              "      <td>1548.902061</td>\n",
              "      <td>152.215221</td>\n",
              "      <td>3855.923006</td>\n",
              "      <td>1471.324043</td>\n",
              "      <td>1189.786399</td>\n",
              "      <td>2446.888744</td>\n",
              "      <td>268.515984</td>\n",
              "      <td>1739.994795</td>\n",
              "      <td>873.985313</td>\n",
              "      <td>630.473126</td>\n",
              "      <td>916.509649</td>\n",
              "      <td>282.814948</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38060</th>\n",
              "      <td>2395.637093</td>\n",
              "      <td>750.619797</td>\n",
              "      <td>762.292555</td>\n",
              "      <td>923.700118</td>\n",
              "      <td>534.103033</td>\n",
              "      <td>3983.529451</td>\n",
              "      <td>1667.992006</td>\n",
              "      <td>1304.840124</td>\n",
              "      <td>2582.700226</td>\n",
              "      <td>406.181687</td>\n",
              "      <td>3099.717859</td>\n",
              "      <td>1028.374277</td>\n",
              "      <td>1006.426513</td>\n",
              "      <td>1814.034473</td>\n",
              "      <td>200.815074</td>\n",
              "      <td>2172.922770</td>\n",
              "      <td>719.849828</td>\n",
              "      <td>515.177030</td>\n",
              "      <td>816.382258</td>\n",
              "      <td>86.643218</td>\n",
              "      <td>7835.108218</td>\n",
              "      <td>2849.351527</td>\n",
              "      <td>3157.023852</td>\n",
              "      <td>4653.726945</td>\n",
              "      <td>849.898330</td>\n",
              "      <td>4444.333995</td>\n",
              "      <td>1924.447825</td>\n",
              "      <td>2032.844584</td>\n",
              "      <td>3761.892980</td>\n",
              "      <td>1486.976154</td>\n",
              "      <td>1323.005134</td>\n",
              "      <td>589.526461</td>\n",
              "      <td>496.775226</td>\n",
              "      <td>716.603442</td>\n",
              "      <td>145.244326</td>\n",
              "      <td>2854.421302</td>\n",
              "      <td>1205.373397</td>\n",
              "      <td>1221.353797</td>\n",
              "      <td>2365.324956</td>\n",
              "      <td>683.425938</td>\n",
              "      <td>6662.994700</td>\n",
              "      <td>2105.495855</td>\n",
              "      <td>1963.277776</td>\n",
              "      <td>3866.578330</td>\n",
              "      <td>574.820768</td>\n",
              "      <td>2146.574888</td>\n",
              "      <td>697.445157</td>\n",
              "      <td>692.873415</td>\n",
              "      <td>1019.248142</td>\n",
              "      <td>63.792831</td>\n",
              "      <td>3702.825853</td>\n",
              "      <td>1187.537793</td>\n",
              "      <td>1150.676827</td>\n",
              "      <td>1538.618951</td>\n",
              "      <td>106.461722</td>\n",
              "      <td>2571.674749</td>\n",
              "      <td>898.352129</td>\n",
              "      <td>714.136101</td>\n",
              "      <td>1499.011274</td>\n",
              "      <td>285.623843</td>\n",
              "      <td>3992.277007</td>\n",
              "      <td>1410.967409</td>\n",
              "      <td>1153.867093</td>\n",
              "      <td>2387.101710</td>\n",
              "      <td>393.105974</td>\n",
              "      <td>1750.557105</td>\n",
              "      <td>852.445441</td>\n",
              "      <td>686.571957</td>\n",
              "      <td>926.301655</td>\n",
              "      <td>75.273107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38061</th>\n",
              "      <td>2543.930835</td>\n",
              "      <td>743.497317</td>\n",
              "      <td>646.164483</td>\n",
              "      <td>917.479178</td>\n",
              "      <td>68.768948</td>\n",
              "      <td>3966.448772</td>\n",
              "      <td>1694.486517</td>\n",
              "      <td>1273.485061</td>\n",
              "      <td>2622.927777</td>\n",
              "      <td>345.181832</td>\n",
              "      <td>3166.770695</td>\n",
              "      <td>1026.931045</td>\n",
              "      <td>885.378375</td>\n",
              "      <td>1769.491161</td>\n",
              "      <td>227.353671</td>\n",
              "      <td>2147.510960</td>\n",
              "      <td>689.503271</td>\n",
              "      <td>480.298696</td>\n",
              "      <td>868.952369</td>\n",
              "      <td>197.803637</td>\n",
              "      <td>7642.024031</td>\n",
              "      <td>2850.021100</td>\n",
              "      <td>3200.971079</td>\n",
              "      <td>4694.727786</td>\n",
              "      <td>306.729658</td>\n",
              "      <td>4317.674897</td>\n",
              "      <td>1798.073632</td>\n",
              "      <td>2085.751984</td>\n",
              "      <td>3772.591272</td>\n",
              "      <td>233.895790</td>\n",
              "      <td>1337.159059</td>\n",
              "      <td>574.320192</td>\n",
              "      <td>466.411956</td>\n",
              "      <td>733.522879</td>\n",
              "      <td>220.498866</td>\n",
              "      <td>2838.084097</td>\n",
              "      <td>1200.108160</td>\n",
              "      <td>1339.991297</td>\n",
              "      <td>2326.901018</td>\n",
              "      <td>210.163528</td>\n",
              "      <td>6502.520480</td>\n",
              "      <td>2135.150530</td>\n",
              "      <td>1889.518433</td>\n",
              "      <td>4026.963838</td>\n",
              "      <td>551.023168</td>\n",
              "      <td>2200.214879</td>\n",
              "      <td>608.956078</td>\n",
              "      <td>707.110094</td>\n",
              "      <td>992.183475</td>\n",
              "      <td>129.670742</td>\n",
              "      <td>3699.830522</td>\n",
              "      <td>1207.013233</td>\n",
              "      <td>1085.577096</td>\n",
              "      <td>1547.847439</td>\n",
              "      <td>321.481557</td>\n",
              "      <td>2498.461512</td>\n",
              "      <td>915.656926</td>\n",
              "      <td>706.836693</td>\n",
              "      <td>1473.166881</td>\n",
              "      <td>254.647793</td>\n",
              "      <td>3878.042699</td>\n",
              "      <td>1444.000266</td>\n",
              "      <td>1121.305468</td>\n",
              "      <td>2434.105212</td>\n",
              "      <td>307.792102</td>\n",
              "      <td>1566.626640</td>\n",
              "      <td>849.635610</td>\n",
              "      <td>706.293838</td>\n",
              "      <td>940.194850</td>\n",
              "      <td>115.098726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38062</th>\n",
              "      <td>2366.175839</td>\n",
              "      <td>708.320663</td>\n",
              "      <td>653.919655</td>\n",
              "      <td>928.671111</td>\n",
              "      <td>251.661687</td>\n",
              "      <td>3979.002095</td>\n",
              "      <td>1616.389767</td>\n",
              "      <td>1354.792909</td>\n",
              "      <td>2527.360255</td>\n",
              "      <td>630.655362</td>\n",
              "      <td>3043.196829</td>\n",
              "      <td>1012.723810</td>\n",
              "      <td>862.076902</td>\n",
              "      <td>1802.593622</td>\n",
              "      <td>115.270877</td>\n",
              "      <td>1982.025318</td>\n",
              "      <td>680.812024</td>\n",
              "      <td>535.285281</td>\n",
              "      <td>850.891613</td>\n",
              "      <td>146.852233</td>\n",
              "      <td>7500.601357</td>\n",
              "      <td>2948.612895</td>\n",
              "      <td>3223.357762</td>\n",
              "      <td>4758.446704</td>\n",
              "      <td>299.083984</td>\n",
              "      <td>4387.835567</td>\n",
              "      <td>1777.256141</td>\n",
              "      <td>2119.552755</td>\n",
              "      <td>3777.865693</td>\n",
              "      <td>1196.108760</td>\n",
              "      <td>1430.941153</td>\n",
              "      <td>550.980445</td>\n",
              "      <td>512.395427</td>\n",
              "      <td>726.180645</td>\n",
              "      <td>154.749261</td>\n",
              "      <td>2862.311163</td>\n",
              "      <td>1109.424542</td>\n",
              "      <td>1354.136675</td>\n",
              "      <td>2362.823026</td>\n",
              "      <td>628.483869</td>\n",
              "      <td>6557.421253</td>\n",
              "      <td>2001.065347</td>\n",
              "      <td>1983.992572</td>\n",
              "      <td>3952.876953</td>\n",
              "      <td>479.930958</td>\n",
              "      <td>2155.716876</td>\n",
              "      <td>653.303303</td>\n",
              "      <td>636.965911</td>\n",
              "      <td>1003.807414</td>\n",
              "      <td>361.577854</td>\n",
              "      <td>3697.372753</td>\n",
              "      <td>1125.542707</td>\n",
              "      <td>1066.194551</td>\n",
              "      <td>1550.252237</td>\n",
              "      <td>154.652258</td>\n",
              "      <td>2619.378534</td>\n",
              "      <td>850.414383</td>\n",
              "      <td>737.710146</td>\n",
              "      <td>1422.594048</td>\n",
              "      <td>162.237152</td>\n",
              "      <td>3930.367997</td>\n",
              "      <td>1391.240069</td>\n",
              "      <td>1124.996419</td>\n",
              "      <td>2309.532619</td>\n",
              "      <td>362.858189</td>\n",
              "      <td>1502.304088</td>\n",
              "      <td>886.827511</td>\n",
              "      <td>691.040133</td>\n",
              "      <td>897.135037</td>\n",
              "      <td>42.467866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38063</th>\n",
              "      <td>2353.662404</td>\n",
              "      <td>732.674939</td>\n",
              "      <td>640.704062</td>\n",
              "      <td>969.503632</td>\n",
              "      <td>179.293337</td>\n",
              "      <td>3857.683046</td>\n",
              "      <td>1601.613756</td>\n",
              "      <td>1354.149427</td>\n",
              "      <td>2683.511277</td>\n",
              "      <td>1272.501427</td>\n",
              "      <td>2965.063607</td>\n",
              "      <td>1044.695360</td>\n",
              "      <td>845.136737</td>\n",
              "      <td>1823.814702</td>\n",
              "      <td>357.189096</td>\n",
              "      <td>1776.910387</td>\n",
              "      <td>661.740586</td>\n",
              "      <td>561.067096</td>\n",
              "      <td>924.698868</td>\n",
              "      <td>634.911430</td>\n",
              "      <td>7359.731243</td>\n",
              "      <td>2863.640066</td>\n",
              "      <td>3400.483367</td>\n",
              "      <td>5194.299139</td>\n",
              "      <td>1959.314782</td>\n",
              "      <td>5175.565533</td>\n",
              "      <td>1606.879470</td>\n",
              "      <td>1836.528805</td>\n",
              "      <td>4104.934124</td>\n",
              "      <td>1315.748064</td>\n",
              "      <td>1578.286258</td>\n",
              "      <td>616.701222</td>\n",
              "      <td>434.778172</td>\n",
              "      <td>672.459809</td>\n",
              "      <td>59.623954</td>\n",
              "      <td>3161.178276</td>\n",
              "      <td>1072.916193</td>\n",
              "      <td>1227.190650</td>\n",
              "      <td>2562.450411</td>\n",
              "      <td>678.151135</td>\n",
              "      <td>6419.721127</td>\n",
              "      <td>1842.812405</td>\n",
              "      <td>1923.984292</td>\n",
              "      <td>4113.429633</td>\n",
              "      <td>1283.351988</td>\n",
              "      <td>2118.085361</td>\n",
              "      <td>689.189110</td>\n",
              "      <td>620.820944</td>\n",
              "      <td>1054.112186</td>\n",
              "      <td>248.543250</td>\n",
              "      <td>3188.306169</td>\n",
              "      <td>1147.261791</td>\n",
              "      <td>1044.410041</td>\n",
              "      <td>1691.220412</td>\n",
              "      <td>711.509493</td>\n",
              "      <td>2297.843070</td>\n",
              "      <td>835.808461</td>\n",
              "      <td>803.033392</td>\n",
              "      <td>1596.578685</td>\n",
              "      <td>634.807983</td>\n",
              "      <td>3703.270095</td>\n",
              "      <td>1300.270596</td>\n",
              "      <td>1254.625884</td>\n",
              "      <td>2595.049176</td>\n",
              "      <td>1178.397358</td>\n",
              "      <td>1391.583953</td>\n",
              "      <td>828.538002</td>\n",
              "      <td>644.685380</td>\n",
              "      <td>908.050390</td>\n",
              "      <td>117.625847</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>38064 rows Ã— 70 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0            1           2  ...          67          68          69\n",
              "0      2631.150479  1358.957902  982.274705  ...  636.610161  851.187212  200.616797\n",
              "1      2581.405819  1483.656365  868.999122  ...  562.980483  823.352860   94.360409\n",
              "2      2558.295428  1610.831205  849.374201  ...  670.962054  793.328897  293.623898\n",
              "3      2582.142347  1637.480330  927.589794  ...  640.150561  803.652406   43.832560\n",
              "4      2416.208172  1661.402028  887.095363  ...  663.840495  876.780429  488.449395\n",
              "...            ...          ...         ...  ...         ...         ...         ...\n",
              "38059  2422.256366   736.276173  739.160028  ...  630.473126  916.509649  282.814948\n",
              "38060  2395.637093   750.619797  762.292555  ...  686.571957  926.301655   75.273107\n",
              "38061  2543.930835   743.497317  646.164483  ...  706.293838  940.194850  115.098726\n",
              "38062  2366.175839   708.320663  653.919655  ...  691.040133  897.135037   42.467866\n",
              "38063  2353.662404   732.674939  640.704062  ...  644.685380  908.050390  117.625847\n",
              "\n",
              "[38064 rows x 70 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tvu_bUN6hCPF",
        "outputId": "29f6151a-1aea-4c68-ead0-d6d25c4fd3e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "label=pd.read_csv(\"label.csv\")\n",
        "label"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.10</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.10</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.10</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.10</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.10</td>\n",
              "      <td>7.31</td>\n",
              "      <td>7.28</td>\n",
              "      <td>8.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38059</th>\n",
              "      <td>6.05</td>\n",
              "      <td>6.67</td>\n",
              "      <td>6.94</td>\n",
              "      <td>6.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38060</th>\n",
              "      <td>6.05</td>\n",
              "      <td>6.67</td>\n",
              "      <td>6.94</td>\n",
              "      <td>6.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38061</th>\n",
              "      <td>6.05</td>\n",
              "      <td>6.67</td>\n",
              "      <td>6.94</td>\n",
              "      <td>6.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38062</th>\n",
              "      <td>6.05</td>\n",
              "      <td>6.67</td>\n",
              "      <td>6.94</td>\n",
              "      <td>6.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38063</th>\n",
              "      <td>6.05</td>\n",
              "      <td>6.67</td>\n",
              "      <td>6.94</td>\n",
              "      <td>6.53</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>38064 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0     1     2     3\n",
              "0      8.10  7.31  7.28  8.47\n",
              "1      8.10  7.31  7.28  8.47\n",
              "2      8.10  7.31  7.28  8.47\n",
              "3      8.10  7.31  7.28  8.47\n",
              "4      8.10  7.31  7.28  8.47\n",
              "...     ...   ...   ...   ...\n",
              "38059  6.05  6.67  6.94  6.53\n",
              "38060  6.05  6.67  6.94  6.53\n",
              "38061  6.05  6.67  6.94  6.53\n",
              "38062  6.05  6.67  6.94  6.53\n",
              "38063  6.05  6.67  6.94  6.53\n",
              "\n",
              "[38064 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWFUQrMuhFej",
        "outputId": "338b881a-443d-4abb-e026-3b1cb403cb32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_valence=label.loc[:,'0']\n",
        "y_valence"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        8.10\n",
              "1        8.10\n",
              "2        8.10\n",
              "3        8.10\n",
              "4        8.10\n",
              "         ... \n",
              "38059    6.05\n",
              "38060    6.05\n",
              "38061    6.05\n",
              "38062    6.05\n",
              "38063    6.05\n",
              "Name: 0, Length: 38064, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GtSXJDZhIhn",
        "outputId": "574417d3-7e2f-4099-ef85-9af459b721eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x=data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x)\n",
        "x = scaler.transform(x)\n",
        "print(x)\n",
        "\n",
        "\n",
        "y_val = to_categorical(y_valence)\n",
        "y_val"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.51158284  0.85952642  0.51960918 ... -0.65163891 -0.96421364\n",
            "  -0.24588982]\n",
            " [ 0.45323289  1.25478335 -0.0329439  ... -1.07960455 -1.04095438\n",
            "  -0.884279  ]\n",
            " [ 0.42612464  1.65788966 -0.12867336 ... -0.45197173 -1.12373199\n",
            "   0.31289756]\n",
            " ...\n",
            " [ 0.40927513 -1.09130004 -1.119921   ... -0.24660902 -0.71881501\n",
            "  -0.75968304]\n",
            " [ 0.20077041 -1.20279954 -1.08209162 ... -0.33526976 -0.83753312\n",
            "  -1.1960498 ]\n",
            " [ 0.18609228 -1.12560375 -1.14655668 ... -0.60470245 -0.80743894\n",
            "  -0.74450007]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cjn1ttYhJLt"
      },
      "source": [
        "x = np.reshape(x, (x.shape[0],1,x.shape[1]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXBa5MelhLot"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y_val, test_size = 0.2, random_state = 4)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX4E3DNahOn_",
        "outputId": "b7427446-2547-4fb8-99db-0906bdd31adb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import LSTM,BatchNormalization,Activation\n",
        "model = Sequential()\n",
        "model.add(LSTM(512, batch_input_shape = (None, None, x.shape[2]),return_sequences=True))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(512, activation='relu',return_sequences=True))\n",
        "#model.add(Dropout(0.1))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(256,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "model.add(LSTM(128,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(LSTM(64,activation=\"relu\",return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "\n",
        "model.add(LSTM(32,activation=\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "rmsprop =keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08)\n",
        "model.compile(loss='mean_squared_error',\n",
        "                  optimizer=rmsprop,\n",
        "                  metrics=['accuracy'])\n",
        "#adam = keras.optimizers.Adam(lr=0.5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, None, 512)         1193984   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, None, 512)         2099200   \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 512)         2048      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, None, 512)         0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, None, 256)         787456    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, None, 256)         1024      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, None, 256)         0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, None, 128)         197120    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, None, 128)         512       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, None, 64)          49408     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, None, 64)          256       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 32)                12416     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 4,345,930\n",
            "Trainable params: 4,342,922\n",
            "Non-trainable params: 3,008\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDBrSSxfhRd3",
        "outputId": "1df56715-ccd5-4990-e84e-b8e33817033c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs = 900, batch_size=150,validation_data= (x_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.2784 - accuracy: 0.1027 - val_loss: 0.2158 - val_accuracy: 0.0775\n",
            "Epoch 2/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.2499 - accuracy: 0.1161 - val_loss: 0.1899 - val_accuracy: 0.0775\n",
            "Epoch 3/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.2252 - accuracy: 0.1265 - val_loss: 0.1925 - val_accuracy: 0.0876\n",
            "Epoch 4/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.2008 - accuracy: 0.1385 - val_loss: 0.1774 - val_accuracy: 0.1363\n",
            "Epoch 5/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.1776 - accuracy: 0.1559 - val_loss: 0.1592 - val_accuracy: 0.2074\n",
            "Epoch 6/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.1565 - accuracy: 0.1553 - val_loss: 0.1407 - val_accuracy: 0.2050\n",
            "Epoch 7/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.1382 - accuracy: 0.1669 - val_loss: 0.1240 - val_accuracy: 0.2041\n",
            "Epoch 8/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.1238 - accuracy: 0.1746 - val_loss: 0.1115 - val_accuracy: 0.2041\n",
            "Epoch 9/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.1138 - accuracy: 0.1801 - val_loss: 0.1050 - val_accuracy: 0.2046\n",
            "Epoch 10/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.1056 - accuracy: 0.1892 - val_loss: 0.0989 - val_accuracy: 0.2075\n",
            "Epoch 11/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.1013 - accuracy: 0.2055 - val_loss: 0.0991 - val_accuracy: 0.2456\n",
            "Epoch 12/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.0980 - accuracy: 0.2186 - val_loss: 0.0940 - val_accuracy: 0.2427\n",
            "Epoch 13/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0972 - accuracy: 0.2302 - val_loss: 0.0922 - val_accuracy: 0.2455\n",
            "Epoch 14/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0954 - accuracy: 0.2362 - val_loss: 0.0913 - val_accuracy: 0.2552\n",
            "Epoch 15/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.0946 - accuracy: 0.2381 - val_loss: 0.0915 - val_accuracy: 0.2603\n",
            "Epoch 16/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.0940 - accuracy: 0.2455 - val_loss: 0.0904 - val_accuracy: 0.2610\n",
            "Epoch 17/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0943 - accuracy: 0.2454 - val_loss: 0.0930 - val_accuracy: 0.2630\n",
            "Epoch 18/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.0932 - accuracy: 0.2516 - val_loss: 0.0897 - val_accuracy: 0.2666\n",
            "Epoch 19/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.0934 - accuracy: 0.2548 - val_loss: 0.0898 - val_accuracy: 0.2651\n",
            "Epoch 20/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0928 - accuracy: 0.2571 - val_loss: 0.0901 - val_accuracy: 0.2670\n",
            "Epoch 21/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0926 - accuracy: 0.2593 - val_loss: 0.0886 - val_accuracy: 0.2790\n",
            "Epoch 22/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0921 - accuracy: 0.2651 - val_loss: 0.0881 - val_accuracy: 0.2790\n",
            "Epoch 23/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0914 - accuracy: 0.2706 - val_loss: 0.0873 - val_accuracy: 0.2915\n",
            "Epoch 24/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0913 - accuracy: 0.2754 - val_loss: 0.0884 - val_accuracy: 0.2974\n",
            "Epoch 25/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0906 - accuracy: 0.2763 - val_loss: 0.0871 - val_accuracy: 0.2979\n",
            "Epoch 26/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0901 - accuracy: 0.2826 - val_loss: 0.0860 - val_accuracy: 0.3129\n",
            "Epoch 27/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0900 - accuracy: 0.2906 - val_loss: 0.0861 - val_accuracy: 0.3202\n",
            "Epoch 28/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0897 - accuracy: 0.2935 - val_loss: 0.0846 - val_accuracy: 0.3283\n",
            "Epoch 29/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0890 - accuracy: 0.2955 - val_loss: 0.0843 - val_accuracy: 0.3377\n",
            "Epoch 30/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0894 - accuracy: 0.3004 - val_loss: 0.0835 - val_accuracy: 0.3427\n",
            "Epoch 31/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0883 - accuracy: 0.3067 - val_loss: 0.0826 - val_accuracy: 0.3498\n",
            "Epoch 32/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0878 - accuracy: 0.3094 - val_loss: 0.0833 - val_accuracy: 0.3535\n",
            "Epoch 33/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0878 - accuracy: 0.3133 - val_loss: 0.0841 - val_accuracy: 0.3535\n",
            "Epoch 34/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0874 - accuracy: 0.3189 - val_loss: 0.0825 - val_accuracy: 0.3583\n",
            "Epoch 35/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0873 - accuracy: 0.3222 - val_loss: 0.0822 - val_accuracy: 0.3610\n",
            "Epoch 36/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0870 - accuracy: 0.3215 - val_loss: 0.0812 - val_accuracy: 0.3666\n",
            "Epoch 37/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0860 - accuracy: 0.3260 - val_loss: 0.0794 - val_accuracy: 0.3737\n",
            "Epoch 38/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0859 - accuracy: 0.3300 - val_loss: 0.0808 - val_accuracy: 0.3737\n",
            "Epoch 39/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0858 - accuracy: 0.3353 - val_loss: 0.0810 - val_accuracy: 0.3742\n",
            "Epoch 40/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0854 - accuracy: 0.3382 - val_loss: 0.0796 - val_accuracy: 0.3887\n",
            "Epoch 41/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0847 - accuracy: 0.3490 - val_loss: 0.0782 - val_accuracy: 0.4002\n",
            "Epoch 42/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0843 - accuracy: 0.3543 - val_loss: 0.0786 - val_accuracy: 0.4126\n",
            "Epoch 43/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0841 - accuracy: 0.3596 - val_loss: 0.0770 - val_accuracy: 0.4151\n",
            "Epoch 44/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0835 - accuracy: 0.3598 - val_loss: 0.0772 - val_accuracy: 0.4264\n",
            "Epoch 45/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0829 - accuracy: 0.3705 - val_loss: 0.0758 - val_accuracy: 0.4322\n",
            "Epoch 46/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0824 - accuracy: 0.3769 - val_loss: 0.0745 - val_accuracy: 0.4390\n",
            "Epoch 47/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0823 - accuracy: 0.3800 - val_loss: 0.0735 - val_accuracy: 0.4416\n",
            "Epoch 48/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0814 - accuracy: 0.3856 - val_loss: 0.0748 - val_accuracy: 0.4406\n",
            "Epoch 49/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0807 - accuracy: 0.3934 - val_loss: 0.0745 - val_accuracy: 0.4553\n",
            "Epoch 50/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0809 - accuracy: 0.3986 - val_loss: 0.0740 - val_accuracy: 0.4544\n",
            "Epoch 51/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0806 - accuracy: 0.4039 - val_loss: 0.0732 - val_accuracy: 0.4617\n",
            "Epoch 52/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0804 - accuracy: 0.4050 - val_loss: 0.0714 - val_accuracy: 0.4710\n",
            "Epoch 53/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0797 - accuracy: 0.4131 - val_loss: 0.0722 - val_accuracy: 0.4825\n",
            "Epoch 54/900\n",
            "204/204 [==============================] - 4s 22ms/step - loss: 0.0790 - accuracy: 0.4193 - val_loss: 0.0716 - val_accuracy: 0.4926\n",
            "Epoch 55/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0787 - accuracy: 0.4265 - val_loss: 0.0703 - val_accuracy: 0.4907\n",
            "Epoch 56/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0783 - accuracy: 0.4317 - val_loss: 0.0696 - val_accuracy: 0.4974\n",
            "Epoch 57/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0780 - accuracy: 0.4399 - val_loss: 0.0684 - val_accuracy: 0.5114\n",
            "Epoch 58/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0774 - accuracy: 0.4418 - val_loss: 0.0678 - val_accuracy: 0.5128\n",
            "Epoch 59/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0770 - accuracy: 0.4509 - val_loss: 0.0689 - val_accuracy: 0.5240\n",
            "Epoch 60/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0762 - accuracy: 0.4556 - val_loss: 0.0674 - val_accuracy: 0.5307\n",
            "Epoch 61/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0762 - accuracy: 0.4610 - val_loss: 0.0673 - val_accuracy: 0.5471\n",
            "Epoch 62/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0755 - accuracy: 0.4697 - val_loss: 0.0640 - val_accuracy: 0.5538\n",
            "Epoch 63/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0749 - accuracy: 0.4774 - val_loss: 0.0652 - val_accuracy: 0.5634\n",
            "Epoch 64/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0740 - accuracy: 0.4855 - val_loss: 0.0650 - val_accuracy: 0.5692\n",
            "Epoch 65/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0736 - accuracy: 0.4939 - val_loss: 0.0621 - val_accuracy: 0.5805\n",
            "Epoch 66/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0733 - accuracy: 0.4961 - val_loss: 0.0612 - val_accuracy: 0.5906\n",
            "Epoch 67/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0729 - accuracy: 0.5028 - val_loss: 0.0615 - val_accuracy: 0.5935\n",
            "Epoch 68/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0721 - accuracy: 0.5091 - val_loss: 0.0589 - val_accuracy: 0.6012\n",
            "Epoch 69/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0710 - accuracy: 0.5202 - val_loss: 0.0603 - val_accuracy: 0.6100\n",
            "Epoch 70/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0705 - accuracy: 0.5257 - val_loss: 0.0582 - val_accuracy: 0.6185\n",
            "Epoch 71/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0700 - accuracy: 0.5335 - val_loss: 0.0568 - val_accuracy: 0.6243\n",
            "Epoch 72/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0694 - accuracy: 0.5384 - val_loss: 0.0567 - val_accuracy: 0.6330\n",
            "Epoch 73/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0687 - accuracy: 0.5490 - val_loss: 0.0565 - val_accuracy: 0.6358\n",
            "Epoch 74/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0683 - accuracy: 0.5515 - val_loss: 0.0555 - val_accuracy: 0.6473\n",
            "Epoch 75/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0675 - accuracy: 0.5580 - val_loss: 0.0545 - val_accuracy: 0.6492\n",
            "Epoch 76/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0668 - accuracy: 0.5647 - val_loss: 0.0538 - val_accuracy: 0.6514\n",
            "Epoch 77/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0666 - accuracy: 0.5734 - val_loss: 0.0507 - val_accuracy: 0.6620\n",
            "Epoch 78/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0653 - accuracy: 0.5791 - val_loss: 0.0515 - val_accuracy: 0.6678\n",
            "Epoch 79/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0652 - accuracy: 0.5831 - val_loss: 0.0525 - val_accuracy: 0.6635\n",
            "Epoch 80/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0649 - accuracy: 0.5827 - val_loss: 0.0509 - val_accuracy: 0.6715\n",
            "Epoch 81/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0637 - accuracy: 0.5973 - val_loss: 0.0502 - val_accuracy: 0.6782\n",
            "Epoch 82/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0633 - accuracy: 0.5979 - val_loss: 0.0474 - val_accuracy: 0.6857\n",
            "Epoch 83/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0630 - accuracy: 0.6026 - val_loss: 0.0486 - val_accuracy: 0.6941\n",
            "Epoch 84/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0620 - accuracy: 0.6065 - val_loss: 0.0462 - val_accuracy: 0.6993\n",
            "Epoch 85/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0619 - accuracy: 0.6133 - val_loss: 0.0464 - val_accuracy: 0.7035\n",
            "Epoch 86/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0609 - accuracy: 0.6190 - val_loss: 0.0475 - val_accuracy: 0.7067\n",
            "Epoch 87/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0600 - accuracy: 0.6296 - val_loss: 0.0457 - val_accuracy: 0.7084\n",
            "Epoch 88/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0600 - accuracy: 0.6312 - val_loss: 0.0445 - val_accuracy: 0.7218\n",
            "Epoch 89/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0594 - accuracy: 0.6326 - val_loss: 0.0412 - val_accuracy: 0.7274\n",
            "Epoch 90/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0590 - accuracy: 0.6373 - val_loss: 0.0430 - val_accuracy: 0.7374\n",
            "Epoch 91/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0585 - accuracy: 0.6455 - val_loss: 0.0424 - val_accuracy: 0.7335\n",
            "Epoch 92/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0572 - accuracy: 0.6523 - val_loss: 0.0431 - val_accuracy: 0.7373\n",
            "Epoch 93/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0566 - accuracy: 0.6556 - val_loss: 0.0390 - val_accuracy: 0.7466\n",
            "Epoch 94/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0563 - accuracy: 0.6584 - val_loss: 0.0387 - val_accuracy: 0.7529\n",
            "Epoch 95/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0560 - accuracy: 0.6641 - val_loss: 0.0391 - val_accuracy: 0.7591\n",
            "Epoch 96/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0556 - accuracy: 0.6660 - val_loss: 0.0383 - val_accuracy: 0.7584\n",
            "Epoch 97/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0553 - accuracy: 0.6695 - val_loss: 0.0378 - val_accuracy: 0.7641\n",
            "Epoch 98/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0539 - accuracy: 0.6799 - val_loss: 0.0381 - val_accuracy: 0.7667\n",
            "Epoch 99/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0536 - accuracy: 0.6818 - val_loss: 0.0386 - val_accuracy: 0.7680\n",
            "Epoch 100/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0536 - accuracy: 0.6856 - val_loss: 0.0373 - val_accuracy: 0.7729\n",
            "Epoch 101/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0524 - accuracy: 0.6924 - val_loss: 0.0373 - val_accuracy: 0.7760\n",
            "Epoch 102/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0512 - accuracy: 0.6982 - val_loss: 0.0346 - val_accuracy: 0.7826\n",
            "Epoch 103/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0513 - accuracy: 0.7013 - val_loss: 0.0334 - val_accuracy: 0.7875\n",
            "Epoch 104/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0513 - accuracy: 0.7018 - val_loss: 0.0351 - val_accuracy: 0.7918\n",
            "Epoch 105/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0504 - accuracy: 0.7077 - val_loss: 0.0340 - val_accuracy: 0.7925\n",
            "Epoch 106/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0500 - accuracy: 0.7097 - val_loss: 0.0332 - val_accuracy: 0.7965\n",
            "Epoch 107/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0494 - accuracy: 0.7143 - val_loss: 0.0323 - val_accuracy: 0.8007\n",
            "Epoch 108/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0488 - accuracy: 0.7183 - val_loss: 0.0324 - val_accuracy: 0.8059\n",
            "Epoch 109/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0485 - accuracy: 0.7257 - val_loss: 0.0311 - val_accuracy: 0.8086\n",
            "Epoch 110/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0481 - accuracy: 0.7226 - val_loss: 0.0315 - val_accuracy: 0.8114\n",
            "Epoch 111/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0474 - accuracy: 0.7310 - val_loss: 0.0311 - val_accuracy: 0.8136\n",
            "Epoch 112/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0472 - accuracy: 0.7313 - val_loss: 0.0318 - val_accuracy: 0.8152\n",
            "Epoch 113/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0469 - accuracy: 0.7348 - val_loss: 0.0311 - val_accuracy: 0.8185\n",
            "Epoch 114/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0454 - accuracy: 0.7431 - val_loss: 0.0299 - val_accuracy: 0.8229\n",
            "Epoch 115/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0456 - accuracy: 0.7421 - val_loss: 0.0299 - val_accuracy: 0.8242\n",
            "Epoch 116/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0448 - accuracy: 0.7483 - val_loss: 0.0300 - val_accuracy: 0.8288\n",
            "Epoch 117/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0447 - accuracy: 0.7479 - val_loss: 0.0274 - val_accuracy: 0.8319\n",
            "Epoch 118/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0447 - accuracy: 0.7507 - val_loss: 0.0319 - val_accuracy: 0.8302\n",
            "Epoch 119/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0441 - accuracy: 0.7522 - val_loss: 0.0275 - val_accuracy: 0.8378\n",
            "Epoch 120/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0431 - accuracy: 0.7569 - val_loss: 0.0274 - val_accuracy: 0.8340\n",
            "Epoch 121/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0435 - accuracy: 0.7576 - val_loss: 0.0266 - val_accuracy: 0.8412\n",
            "Epoch 122/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0422 - accuracy: 0.7663 - val_loss: 0.0279 - val_accuracy: 0.8474\n",
            "Epoch 123/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0422 - accuracy: 0.7659 - val_loss: 0.0249 - val_accuracy: 0.8489\n",
            "Epoch 124/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0419 - accuracy: 0.7683 - val_loss: 0.0262 - val_accuracy: 0.8478\n",
            "Epoch 125/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0411 - accuracy: 0.7723 - val_loss: 0.0258 - val_accuracy: 0.8483\n",
            "Epoch 126/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0400 - accuracy: 0.7765 - val_loss: 0.0249 - val_accuracy: 0.8550\n",
            "Epoch 127/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0389 - accuracy: 0.7807 - val_loss: 0.0251 - val_accuracy: 0.8537\n",
            "Epoch 128/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0383 - accuracy: 0.7809 - val_loss: 0.0236 - val_accuracy: 0.8580\n",
            "Epoch 129/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0372 - accuracy: 0.7849 - val_loss: 0.0231 - val_accuracy: 0.8631\n",
            "Epoch 130/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0364 - accuracy: 0.7876 - val_loss: 0.0228 - val_accuracy: 0.8631\n",
            "Epoch 131/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0359 - accuracy: 0.7887 - val_loss: 0.0215 - val_accuracy: 0.8676\n",
            "Epoch 132/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0351 - accuracy: 0.7929 - val_loss: 0.0228 - val_accuracy: 0.8677\n",
            "Epoch 133/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0344 - accuracy: 0.7967 - val_loss: 0.0205 - val_accuracy: 0.8694\n",
            "Epoch 134/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0342 - accuracy: 0.7943 - val_loss: 0.0212 - val_accuracy: 0.8701\n",
            "Epoch 135/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0336 - accuracy: 0.8016 - val_loss: 0.0201 - val_accuracy: 0.8756\n",
            "Epoch 136/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0332 - accuracy: 0.8018 - val_loss: 0.0187 - val_accuracy: 0.8834\n",
            "Epoch 137/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0325 - accuracy: 0.8047 - val_loss: 0.0196 - val_accuracy: 0.8774\n",
            "Epoch 138/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0314 - accuracy: 0.8109 - val_loss: 0.0188 - val_accuracy: 0.8835\n",
            "Epoch 139/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0313 - accuracy: 0.8127 - val_loss: 0.0195 - val_accuracy: 0.8802\n",
            "Epoch 140/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0308 - accuracy: 0.8150 - val_loss: 0.0183 - val_accuracy: 0.8834\n",
            "Epoch 141/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0304 - accuracy: 0.8165 - val_loss: 0.0175 - val_accuracy: 0.8868\n",
            "Epoch 142/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0295 - accuracy: 0.8230 - val_loss: 0.0181 - val_accuracy: 0.8899\n",
            "Epoch 143/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0296 - accuracy: 0.8214 - val_loss: 0.0183 - val_accuracy: 0.8852\n",
            "Epoch 144/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0291 - accuracy: 0.8255 - val_loss: 0.0185 - val_accuracy: 0.8899\n",
            "Epoch 145/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0287 - accuracy: 0.8253 - val_loss: 0.0171 - val_accuracy: 0.8905\n",
            "Epoch 146/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0279 - accuracy: 0.8315 - val_loss: 0.0169 - val_accuracy: 0.8919\n",
            "Epoch 147/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0277 - accuracy: 0.8294 - val_loss: 0.0177 - val_accuracy: 0.8933\n",
            "Epoch 148/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0273 - accuracy: 0.8348 - val_loss: 0.0166 - val_accuracy: 0.8971\n",
            "Epoch 149/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0271 - accuracy: 0.8361 - val_loss: 0.0176 - val_accuracy: 0.8974\n",
            "Epoch 150/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0271 - accuracy: 0.8359 - val_loss: 0.0159 - val_accuracy: 0.8989\n",
            "Epoch 151/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0266 - accuracy: 0.8399 - val_loss: 0.0149 - val_accuracy: 0.9049\n",
            "Epoch 152/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0259 - accuracy: 0.8418 - val_loss: 0.0146 - val_accuracy: 0.9056\n",
            "Epoch 153/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0258 - accuracy: 0.8415 - val_loss: 0.0161 - val_accuracy: 0.9011\n",
            "Epoch 154/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0254 - accuracy: 0.8440 - val_loss: 0.0148 - val_accuracy: 0.9060\n",
            "Epoch 155/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0254 - accuracy: 0.8453 - val_loss: 0.0143 - val_accuracy: 0.9105\n",
            "Epoch 156/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0255 - accuracy: 0.8434 - val_loss: 0.0148 - val_accuracy: 0.9098\n",
            "Epoch 157/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0241 - accuracy: 0.8529 - val_loss: 0.0144 - val_accuracy: 0.9109\n",
            "Epoch 158/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0246 - accuracy: 0.8513 - val_loss: 0.0135 - val_accuracy: 0.9138\n",
            "Epoch 159/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0242 - accuracy: 0.8512 - val_loss: 0.0132 - val_accuracy: 0.9166\n",
            "Epoch 160/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0236 - accuracy: 0.8570 - val_loss: 0.0129 - val_accuracy: 0.9153\n",
            "Epoch 161/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0234 - accuracy: 0.8571 - val_loss: 0.0139 - val_accuracy: 0.9166\n",
            "Epoch 162/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0231 - accuracy: 0.8599 - val_loss: 0.0129 - val_accuracy: 0.9213\n",
            "Epoch 163/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0230 - accuracy: 0.8611 - val_loss: 0.0123 - val_accuracy: 0.9211\n",
            "Epoch 164/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0228 - accuracy: 0.8628 - val_loss: 0.0126 - val_accuracy: 0.9224\n",
            "Epoch 165/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0227 - accuracy: 0.8643 - val_loss: 0.0135 - val_accuracy: 0.9205\n",
            "Epoch 166/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0224 - accuracy: 0.8635 - val_loss: 0.0124 - val_accuracy: 0.9229\n",
            "Epoch 167/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0218 - accuracy: 0.8686 - val_loss: 0.0125 - val_accuracy: 0.9266\n",
            "Epoch 168/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0218 - accuracy: 0.8684 - val_loss: 0.0125 - val_accuracy: 0.9264\n",
            "Epoch 169/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0220 - accuracy: 0.8663 - val_loss: 0.0114 - val_accuracy: 0.9291\n",
            "Epoch 170/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0213 - accuracy: 0.8728 - val_loss: 0.0117 - val_accuracy: 0.9297\n",
            "Epoch 171/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0211 - accuracy: 0.8725 - val_loss: 0.0109 - val_accuracy: 0.9297\n",
            "Epoch 172/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0208 - accuracy: 0.8749 - val_loss: 0.0114 - val_accuracy: 0.9317\n",
            "Epoch 173/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0204 - accuracy: 0.8784 - val_loss: 0.0104 - val_accuracy: 0.9337\n",
            "Epoch 174/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0207 - accuracy: 0.8746 - val_loss: 0.0111 - val_accuracy: 0.9330\n",
            "Epoch 175/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0205 - accuracy: 0.8755 - val_loss: 0.0104 - val_accuracy: 0.9360\n",
            "Epoch 176/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0198 - accuracy: 0.8812 - val_loss: 0.0112 - val_accuracy: 0.9366\n",
            "Epoch 177/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0198 - accuracy: 0.8811 - val_loss: 0.0114 - val_accuracy: 0.9339\n",
            "Epoch 178/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0196 - accuracy: 0.8823 - val_loss: 0.0101 - val_accuracy: 0.9380\n",
            "Epoch 179/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0197 - accuracy: 0.8827 - val_loss: 0.0102 - val_accuracy: 0.9367\n",
            "Epoch 180/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0195 - accuracy: 0.8827 - val_loss: 0.0108 - val_accuracy: 0.9352\n",
            "Epoch 181/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0192 - accuracy: 0.8842 - val_loss: 0.0103 - val_accuracy: 0.9368\n",
            "Epoch 182/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0190 - accuracy: 0.8858 - val_loss: 0.0117 - val_accuracy: 0.9355\n",
            "Epoch 183/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0190 - accuracy: 0.8861 - val_loss: 0.0101 - val_accuracy: 0.9406\n",
            "Epoch 184/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0186 - accuracy: 0.8885 - val_loss: 0.0109 - val_accuracy: 0.9398\n",
            "Epoch 185/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0187 - accuracy: 0.8868 - val_loss: 0.0111 - val_accuracy: 0.9375\n",
            "Epoch 186/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0183 - accuracy: 0.8889 - val_loss: 0.0092 - val_accuracy: 0.9433\n",
            "Epoch 187/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0181 - accuracy: 0.8922 - val_loss: 0.0095 - val_accuracy: 0.9419\n",
            "Epoch 188/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0178 - accuracy: 0.8937 - val_loss: 0.0091 - val_accuracy: 0.9419\n",
            "Epoch 189/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0175 - accuracy: 0.8958 - val_loss: 0.0095 - val_accuracy: 0.9438\n",
            "Epoch 190/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0176 - accuracy: 0.8955 - val_loss: 0.0091 - val_accuracy: 0.9450\n",
            "Epoch 191/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0176 - accuracy: 0.8954 - val_loss: 0.0085 - val_accuracy: 0.9482\n",
            "Epoch 192/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0171 - accuracy: 0.8980 - val_loss: 0.0086 - val_accuracy: 0.9456\n",
            "Epoch 193/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0171 - accuracy: 0.8980 - val_loss: 0.0099 - val_accuracy: 0.9415\n",
            "Epoch 194/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0172 - accuracy: 0.8966 - val_loss: 0.0090 - val_accuracy: 0.9477\n",
            "Epoch 195/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0172 - accuracy: 0.8969 - val_loss: 0.0091 - val_accuracy: 0.9463\n",
            "Epoch 196/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0165 - accuracy: 0.9011 - val_loss: 0.0094 - val_accuracy: 0.9454\n",
            "Epoch 197/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0168 - accuracy: 0.8989 - val_loss: 0.0083 - val_accuracy: 0.9488\n",
            "Epoch 198/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0164 - accuracy: 0.9006 - val_loss: 0.0089 - val_accuracy: 0.9468\n",
            "Epoch 199/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0161 - accuracy: 0.9038 - val_loss: 0.0080 - val_accuracy: 0.9503\n",
            "Epoch 200/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0161 - accuracy: 0.9039 - val_loss: 0.0089 - val_accuracy: 0.9510\n",
            "Epoch 201/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0162 - accuracy: 0.9039 - val_loss: 0.0079 - val_accuracy: 0.9513\n",
            "Epoch 202/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0157 - accuracy: 0.9066 - val_loss: 0.0079 - val_accuracy: 0.9522\n",
            "Epoch 203/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0161 - accuracy: 0.9035 - val_loss: 0.0082 - val_accuracy: 0.9496\n",
            "Epoch 204/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0154 - accuracy: 0.9082 - val_loss: 0.0076 - val_accuracy: 0.9523\n",
            "Epoch 205/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0150 - accuracy: 0.9097 - val_loss: 0.0072 - val_accuracy: 0.9539\n",
            "Epoch 206/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0154 - accuracy: 0.9084 - val_loss: 0.0075 - val_accuracy: 0.9517\n",
            "Epoch 207/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0154 - accuracy: 0.9085 - val_loss: 0.0079 - val_accuracy: 0.9535\n",
            "Epoch 208/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0151 - accuracy: 0.9105 - val_loss: 0.0079 - val_accuracy: 0.9524\n",
            "Epoch 209/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0151 - accuracy: 0.9100 - val_loss: 0.0077 - val_accuracy: 0.9527\n",
            "Epoch 210/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0149 - accuracy: 0.9112 - val_loss: 0.0076 - val_accuracy: 0.9556\n",
            "Epoch 211/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0148 - accuracy: 0.9104 - val_loss: 0.0071 - val_accuracy: 0.9589\n",
            "Epoch 212/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0148 - accuracy: 0.9130 - val_loss: 0.0075 - val_accuracy: 0.9570\n",
            "Epoch 213/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0143 - accuracy: 0.9142 - val_loss: 0.0079 - val_accuracy: 0.9518\n",
            "Epoch 214/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0146 - accuracy: 0.9128 - val_loss: 0.0076 - val_accuracy: 0.9553\n",
            "Epoch 215/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0147 - accuracy: 0.9118 - val_loss: 0.0069 - val_accuracy: 0.9576\n",
            "Epoch 216/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0141 - accuracy: 0.9155 - val_loss: 0.0069 - val_accuracy: 0.9586\n",
            "Epoch 217/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0138 - accuracy: 0.9171 - val_loss: 0.0070 - val_accuracy: 0.9553\n",
            "Epoch 218/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0137 - accuracy: 0.9197 - val_loss: 0.0072 - val_accuracy: 0.9561\n",
            "Epoch 219/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0142 - accuracy: 0.9146 - val_loss: 0.0069 - val_accuracy: 0.9593\n",
            "Epoch 220/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0140 - accuracy: 0.9165 - val_loss: 0.0070 - val_accuracy: 0.9559\n",
            "Epoch 221/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0138 - accuracy: 0.9166 - val_loss: 0.0069 - val_accuracy: 0.9585\n",
            "Epoch 222/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0134 - accuracy: 0.9206 - val_loss: 0.0071 - val_accuracy: 0.9576\n",
            "Epoch 223/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0131 - accuracy: 0.9226 - val_loss: 0.0064 - val_accuracy: 0.9597\n",
            "Epoch 224/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0132 - accuracy: 0.9225 - val_loss: 0.0063 - val_accuracy: 0.9597\n",
            "Epoch 225/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0134 - accuracy: 0.9206 - val_loss: 0.0068 - val_accuracy: 0.9591\n",
            "Epoch 226/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0127 - accuracy: 0.9251 - val_loss: 0.0064 - val_accuracy: 0.9597\n",
            "Epoch 227/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0127 - accuracy: 0.9251 - val_loss: 0.0063 - val_accuracy: 0.9614\n",
            "Epoch 228/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0133 - accuracy: 0.9213 - val_loss: 0.0064 - val_accuracy: 0.9609\n",
            "Epoch 229/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0129 - accuracy: 0.9225 - val_loss: 0.0062 - val_accuracy: 0.9615\n",
            "Epoch 230/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0130 - accuracy: 0.9233 - val_loss: 0.0061 - val_accuracy: 0.9613\n",
            "Epoch 231/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0129 - accuracy: 0.9241 - val_loss: 0.0059 - val_accuracy: 0.9614\n",
            "Epoch 232/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0128 - accuracy: 0.9242 - val_loss: 0.0060 - val_accuracy: 0.9622\n",
            "Epoch 233/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0121 - accuracy: 0.9286 - val_loss: 0.0061 - val_accuracy: 0.9626\n",
            "Epoch 234/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0125 - accuracy: 0.9262 - val_loss: 0.0060 - val_accuracy: 0.9602\n",
            "Epoch 235/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0123 - accuracy: 0.9271 - val_loss: 0.0061 - val_accuracy: 0.9616\n",
            "Epoch 236/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0120 - accuracy: 0.9290 - val_loss: 0.0060 - val_accuracy: 0.9624\n",
            "Epoch 237/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0123 - accuracy: 0.9263 - val_loss: 0.0059 - val_accuracy: 0.9635\n",
            "Epoch 238/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0119 - accuracy: 0.9301 - val_loss: 0.0058 - val_accuracy: 0.9630\n",
            "Epoch 239/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0121 - accuracy: 0.9286 - val_loss: 0.0057 - val_accuracy: 0.9639\n",
            "Epoch 240/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0121 - accuracy: 0.9278 - val_loss: 0.0056 - val_accuracy: 0.9653\n",
            "Epoch 241/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0115 - accuracy: 0.9329 - val_loss: 0.0054 - val_accuracy: 0.9664\n",
            "Epoch 242/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0119 - accuracy: 0.9303 - val_loss: 0.0057 - val_accuracy: 0.9636\n",
            "Epoch 243/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0114 - accuracy: 0.9323 - val_loss: 0.0054 - val_accuracy: 0.9655\n",
            "Epoch 244/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0118 - accuracy: 0.9299 - val_loss: 0.0053 - val_accuracy: 0.9656\n",
            "Epoch 245/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0114 - accuracy: 0.9332 - val_loss: 0.0053 - val_accuracy: 0.9672\n",
            "Epoch 246/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0115 - accuracy: 0.9328 - val_loss: 0.0056 - val_accuracy: 0.9651\n",
            "Epoch 247/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0115 - accuracy: 0.9324 - val_loss: 0.0056 - val_accuracy: 0.9652\n",
            "Epoch 248/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0115 - accuracy: 0.9325 - val_loss: 0.0054 - val_accuracy: 0.9668\n",
            "Epoch 249/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0113 - accuracy: 0.9337 - val_loss: 0.0054 - val_accuracy: 0.9662\n",
            "Epoch 250/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0111 - accuracy: 0.9351 - val_loss: 0.0051 - val_accuracy: 0.9673\n",
            "Epoch 251/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0112 - accuracy: 0.9349 - val_loss: 0.0051 - val_accuracy: 0.9679\n",
            "Epoch 252/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0112 - accuracy: 0.9338 - val_loss: 0.0053 - val_accuracy: 0.9674\n",
            "Epoch 253/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0108 - accuracy: 0.9365 - val_loss: 0.0049 - val_accuracy: 0.9694\n",
            "Epoch 254/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0108 - accuracy: 0.9363 - val_loss: 0.0049 - val_accuracy: 0.9691\n",
            "Epoch 255/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0110 - accuracy: 0.9348 - val_loss: 0.0056 - val_accuracy: 0.9662\n",
            "Epoch 256/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0109 - accuracy: 0.9355 - val_loss: 0.0048 - val_accuracy: 0.9691\n",
            "Epoch 257/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0106 - accuracy: 0.9377 - val_loss: 0.0047 - val_accuracy: 0.9714\n",
            "Epoch 258/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0108 - accuracy: 0.9365 - val_loss: 0.0048 - val_accuracy: 0.9698\n",
            "Epoch 259/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0106 - accuracy: 0.9380 - val_loss: 0.0045 - val_accuracy: 0.9712\n",
            "Epoch 260/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0104 - accuracy: 0.9395 - val_loss: 0.0049 - val_accuracy: 0.9698\n",
            "Epoch 261/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0106 - accuracy: 0.9373 - val_loss: 0.0050 - val_accuracy: 0.9695\n",
            "Epoch 262/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0104 - accuracy: 0.9384 - val_loss: 0.0044 - val_accuracy: 0.9727\n",
            "Epoch 263/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0100 - accuracy: 0.9409 - val_loss: 0.0045 - val_accuracy: 0.9711\n",
            "Epoch 264/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0103 - accuracy: 0.9399 - val_loss: 0.0049 - val_accuracy: 0.9690\n",
            "Epoch 265/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0103 - accuracy: 0.9401 - val_loss: 0.0049 - val_accuracy: 0.9687\n",
            "Epoch 266/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0099 - accuracy: 0.9423 - val_loss: 0.0045 - val_accuracy: 0.9714\n",
            "Epoch 267/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0102 - accuracy: 0.9405 - val_loss: 0.0047 - val_accuracy: 0.9708\n",
            "Epoch 268/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0101 - accuracy: 0.9411 - val_loss: 0.0046 - val_accuracy: 0.9714\n",
            "Epoch 269/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0100 - accuracy: 0.9405 - val_loss: 0.0045 - val_accuracy: 0.9714\n",
            "Epoch 270/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0102 - accuracy: 0.9408 - val_loss: 0.0045 - val_accuracy: 0.9722\n",
            "Epoch 271/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0103 - accuracy: 0.9406 - val_loss: 0.0046 - val_accuracy: 0.9707\n",
            "Epoch 272/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0095 - accuracy: 0.9453 - val_loss: 0.0042 - val_accuracy: 0.9739\n",
            "Epoch 273/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0100 - accuracy: 0.9417 - val_loss: 0.0041 - val_accuracy: 0.9737\n",
            "Epoch 274/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0098 - accuracy: 0.9430 - val_loss: 0.0043 - val_accuracy: 0.9725\n",
            "Epoch 275/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0100 - accuracy: 0.9417 - val_loss: 0.0041 - val_accuracy: 0.9736\n",
            "Epoch 276/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0096 - accuracy: 0.9445 - val_loss: 0.0046 - val_accuracy: 0.9719\n",
            "Epoch 277/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0096 - accuracy: 0.9434 - val_loss: 0.0044 - val_accuracy: 0.9722\n",
            "Epoch 278/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0093 - accuracy: 0.9454 - val_loss: 0.0040 - val_accuracy: 0.9748\n",
            "Epoch 279/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0097 - accuracy: 0.9423 - val_loss: 0.0042 - val_accuracy: 0.9737\n",
            "Epoch 280/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0094 - accuracy: 0.9452 - val_loss: 0.0043 - val_accuracy: 0.9727\n",
            "Epoch 281/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0093 - accuracy: 0.9457 - val_loss: 0.0042 - val_accuracy: 0.9743\n",
            "Epoch 282/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0096 - accuracy: 0.9430 - val_loss: 0.0041 - val_accuracy: 0.9736\n",
            "Epoch 283/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0090 - accuracy: 0.9482 - val_loss: 0.0042 - val_accuracy: 0.9737\n",
            "Epoch 284/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0093 - accuracy: 0.9458 - val_loss: 0.0043 - val_accuracy: 0.9744\n",
            "Epoch 285/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0094 - accuracy: 0.9454 - val_loss: 0.0043 - val_accuracy: 0.9733\n",
            "Epoch 286/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0094 - accuracy: 0.9446 - val_loss: 0.0042 - val_accuracy: 0.9740\n",
            "Epoch 287/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0094 - accuracy: 0.9458 - val_loss: 0.0043 - val_accuracy: 0.9736\n",
            "Epoch 288/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0088 - accuracy: 0.9487 - val_loss: 0.0041 - val_accuracy: 0.9746\n",
            "Epoch 289/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0091 - accuracy: 0.9465 - val_loss: 0.0039 - val_accuracy: 0.9753\n",
            "Epoch 290/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0092 - accuracy: 0.9471 - val_loss: 0.0042 - val_accuracy: 0.9743\n",
            "Epoch 291/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0089 - accuracy: 0.9474 - val_loss: 0.0041 - val_accuracy: 0.9750\n",
            "Epoch 292/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0088 - accuracy: 0.9480 - val_loss: 0.0043 - val_accuracy: 0.9737\n",
            "Epoch 293/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0089 - accuracy: 0.9483 - val_loss: 0.0043 - val_accuracy: 0.9739\n",
            "Epoch 294/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0090 - accuracy: 0.9482 - val_loss: 0.0041 - val_accuracy: 0.9754\n",
            "Epoch 295/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0087 - accuracy: 0.9493 - val_loss: 0.0040 - val_accuracy: 0.9754\n",
            "Epoch 296/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0087 - accuracy: 0.9491 - val_loss: 0.0039 - val_accuracy: 0.9749\n",
            "Epoch 297/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0085 - accuracy: 0.9501 - val_loss: 0.0039 - val_accuracy: 0.9758\n",
            "Epoch 298/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0085 - accuracy: 0.9506 - val_loss: 0.0039 - val_accuracy: 0.9762\n",
            "Epoch 299/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0084 - accuracy: 0.9510 - val_loss: 0.0044 - val_accuracy: 0.9740\n",
            "Epoch 300/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0088 - accuracy: 0.9489 - val_loss: 0.0041 - val_accuracy: 0.9754\n",
            "Epoch 301/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0087 - accuracy: 0.9493 - val_loss: 0.0039 - val_accuracy: 0.9760\n",
            "Epoch 302/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0085 - accuracy: 0.9503 - val_loss: 0.0040 - val_accuracy: 0.9760\n",
            "Epoch 303/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0083 - accuracy: 0.9518 - val_loss: 0.0043 - val_accuracy: 0.9735\n",
            "Epoch 304/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0088 - accuracy: 0.9477 - val_loss: 0.0041 - val_accuracy: 0.9761\n",
            "Epoch 305/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0082 - accuracy: 0.9529 - val_loss: 0.0039 - val_accuracy: 0.9769\n",
            "Epoch 306/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0082 - accuracy: 0.9518 - val_loss: 0.0044 - val_accuracy: 0.9736\n",
            "Epoch 307/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0078 - accuracy: 0.9555 - val_loss: 0.0041 - val_accuracy: 0.9745\n",
            "Epoch 308/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0085 - accuracy: 0.9498 - val_loss: 0.0040 - val_accuracy: 0.9758\n",
            "Epoch 309/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0081 - accuracy: 0.9538 - val_loss: 0.0040 - val_accuracy: 0.9760\n",
            "Epoch 310/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0083 - accuracy: 0.9517 - val_loss: 0.0037 - val_accuracy: 0.9762\n",
            "Epoch 311/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0081 - accuracy: 0.9534 - val_loss: 0.0041 - val_accuracy: 0.9754\n",
            "Epoch 312/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0080 - accuracy: 0.9536 - val_loss: 0.0041 - val_accuracy: 0.9753\n",
            "Epoch 313/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0082 - accuracy: 0.9526 - val_loss: 0.0038 - val_accuracy: 0.9768\n",
            "Epoch 314/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0081 - accuracy: 0.9523 - val_loss: 0.0040 - val_accuracy: 0.9765\n",
            "Epoch 315/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0082 - accuracy: 0.9524 - val_loss: 0.0041 - val_accuracy: 0.9750\n",
            "Epoch 316/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0080 - accuracy: 0.9531 - val_loss: 0.0041 - val_accuracy: 0.9758\n",
            "Epoch 317/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0081 - accuracy: 0.9523 - val_loss: 0.0039 - val_accuracy: 0.9768\n",
            "Epoch 318/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0078 - accuracy: 0.9550 - val_loss: 0.0040 - val_accuracy: 0.9756\n",
            "Epoch 319/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0078 - accuracy: 0.9547 - val_loss: 0.0042 - val_accuracy: 0.9748\n",
            "Epoch 320/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0076 - accuracy: 0.9559 - val_loss: 0.0040 - val_accuracy: 0.9756\n",
            "Epoch 321/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0075 - accuracy: 0.9572 - val_loss: 0.0038 - val_accuracy: 0.9769\n",
            "Epoch 322/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0077 - accuracy: 0.9553 - val_loss: 0.0038 - val_accuracy: 0.9764\n",
            "Epoch 323/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0079 - accuracy: 0.9536 - val_loss: 0.0040 - val_accuracy: 0.9760\n",
            "Epoch 324/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0076 - accuracy: 0.9561 - val_loss: 0.0038 - val_accuracy: 0.9766\n",
            "Epoch 325/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0074 - accuracy: 0.9572 - val_loss: 0.0037 - val_accuracy: 0.9773\n",
            "Epoch 326/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0074 - accuracy: 0.9574 - val_loss: 0.0038 - val_accuracy: 0.9757\n",
            "Epoch 327/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0075 - accuracy: 0.9569 - val_loss: 0.0037 - val_accuracy: 0.9774\n",
            "Epoch 328/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0075 - accuracy: 0.9565 - val_loss: 0.0037 - val_accuracy: 0.9770\n",
            "Epoch 329/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0075 - accuracy: 0.9566 - val_loss: 0.0039 - val_accuracy: 0.9762\n",
            "Epoch 330/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0077 - accuracy: 0.9545 - val_loss: 0.0039 - val_accuracy: 0.9766\n",
            "Epoch 331/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0075 - accuracy: 0.9556 - val_loss: 0.0038 - val_accuracy: 0.9770\n",
            "Epoch 332/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0076 - accuracy: 0.9550 - val_loss: 0.0038 - val_accuracy: 0.9761\n",
            "Epoch 333/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0076 - accuracy: 0.9556 - val_loss: 0.0037 - val_accuracy: 0.9785\n",
            "Epoch 334/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0074 - accuracy: 0.9578 - val_loss: 0.0035 - val_accuracy: 0.9785\n",
            "Epoch 335/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0073 - accuracy: 0.9591 - val_loss: 0.0037 - val_accuracy: 0.9781\n",
            "Epoch 336/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0074 - accuracy: 0.9570 - val_loss: 0.0037 - val_accuracy: 0.9770\n",
            "Epoch 337/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0073 - accuracy: 0.9572 - val_loss: 0.0042 - val_accuracy: 0.9757\n",
            "Epoch 338/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0073 - accuracy: 0.9582 - val_loss: 0.0041 - val_accuracy: 0.9750\n",
            "Epoch 339/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0072 - accuracy: 0.9589 - val_loss: 0.0037 - val_accuracy: 0.9771\n",
            "Epoch 340/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0073 - accuracy: 0.9584 - val_loss: 0.0036 - val_accuracy: 0.9789\n",
            "Epoch 341/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0072 - accuracy: 0.9582 - val_loss: 0.0036 - val_accuracy: 0.9786\n",
            "Epoch 342/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0071 - accuracy: 0.9591 - val_loss: 0.0039 - val_accuracy: 0.9775\n",
            "Epoch 343/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0072 - accuracy: 0.9585 - val_loss: 0.0037 - val_accuracy: 0.9777\n",
            "Epoch 344/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0073 - accuracy: 0.9578 - val_loss: 0.0035 - val_accuracy: 0.9796\n",
            "Epoch 345/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0071 - accuracy: 0.9584 - val_loss: 0.0037 - val_accuracy: 0.9775\n",
            "Epoch 346/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0071 - accuracy: 0.9590 - val_loss: 0.0036 - val_accuracy: 0.9785\n",
            "Epoch 347/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0074 - accuracy: 0.9575 - val_loss: 0.0035 - val_accuracy: 0.9792\n",
            "Epoch 348/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0071 - accuracy: 0.9593 - val_loss: 0.0035 - val_accuracy: 0.9785\n",
            "Epoch 349/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0068 - accuracy: 0.9608 - val_loss: 0.0037 - val_accuracy: 0.9782\n",
            "Epoch 350/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0069 - accuracy: 0.9603 - val_loss: 0.0038 - val_accuracy: 0.9764\n",
            "Epoch 351/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0068 - accuracy: 0.9613 - val_loss: 0.0038 - val_accuracy: 0.9774\n",
            "Epoch 352/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0071 - accuracy: 0.9588 - val_loss: 0.0036 - val_accuracy: 0.9778\n",
            "Epoch 353/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0071 - accuracy: 0.9588 - val_loss: 0.0036 - val_accuracy: 0.9790\n",
            "Epoch 354/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0070 - accuracy: 0.9588 - val_loss: 0.0035 - val_accuracy: 0.9787\n",
            "Epoch 355/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0068 - accuracy: 0.9601 - val_loss: 0.0032 - val_accuracy: 0.9808\n",
            "Epoch 356/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0067 - accuracy: 0.9618 - val_loss: 0.0036 - val_accuracy: 0.9790\n",
            "Epoch 357/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0071 - accuracy: 0.9583 - val_loss: 0.0036 - val_accuracy: 0.9782\n",
            "Epoch 358/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0068 - accuracy: 0.9609 - val_loss: 0.0039 - val_accuracy: 0.9762\n",
            "Epoch 359/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0067 - accuracy: 0.9611 - val_loss: 0.0036 - val_accuracy: 0.9789\n",
            "Epoch 360/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0067 - accuracy: 0.9609 - val_loss: 0.0034 - val_accuracy: 0.9798\n",
            "Epoch 361/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0068 - accuracy: 0.9610 - val_loss: 0.0037 - val_accuracy: 0.9779\n",
            "Epoch 362/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0068 - accuracy: 0.9609 - val_loss: 0.0036 - val_accuracy: 0.9790\n",
            "Epoch 363/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0067 - accuracy: 0.9610 - val_loss: 0.0034 - val_accuracy: 0.9799\n",
            "Epoch 364/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0067 - accuracy: 0.9606 - val_loss: 0.0035 - val_accuracy: 0.9789\n",
            "Epoch 365/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0066 - accuracy: 0.9612 - val_loss: 0.0035 - val_accuracy: 0.9787\n",
            "Epoch 366/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0066 - accuracy: 0.9621 - val_loss: 0.0036 - val_accuracy: 0.9786\n",
            "Epoch 367/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0064 - accuracy: 0.9627 - val_loss: 0.0034 - val_accuracy: 0.9787\n",
            "Epoch 368/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0067 - accuracy: 0.9608 - val_loss: 0.0033 - val_accuracy: 0.9807\n",
            "Epoch 369/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0066 - accuracy: 0.9610 - val_loss: 0.0037 - val_accuracy: 0.9779\n",
            "Epoch 370/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0065 - accuracy: 0.9622 - val_loss: 0.0037 - val_accuracy: 0.9785\n",
            "Epoch 371/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0066 - accuracy: 0.9615 - val_loss: 0.0036 - val_accuracy: 0.9786\n",
            "Epoch 372/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0064 - accuracy: 0.9636 - val_loss: 0.0036 - val_accuracy: 0.9786\n",
            "Epoch 373/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0069 - accuracy: 0.9603 - val_loss: 0.0036 - val_accuracy: 0.9795\n",
            "Epoch 374/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0065 - accuracy: 0.9617 - val_loss: 0.0034 - val_accuracy: 0.9794\n",
            "Epoch 375/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0063 - accuracy: 0.9633 - val_loss: 0.0034 - val_accuracy: 0.9806\n",
            "Epoch 376/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0064 - accuracy: 0.9627 - val_loss: 0.0037 - val_accuracy: 0.9790\n",
            "Epoch 377/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0064 - accuracy: 0.9632 - val_loss: 0.0035 - val_accuracy: 0.9798\n",
            "Epoch 378/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0064 - accuracy: 0.9628 - val_loss: 0.0036 - val_accuracy: 0.9789\n",
            "Epoch 379/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0064 - accuracy: 0.9628 - val_loss: 0.0032 - val_accuracy: 0.9812\n",
            "Epoch 380/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0062 - accuracy: 0.9639 - val_loss: 0.0032 - val_accuracy: 0.9804\n",
            "Epoch 381/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0064 - accuracy: 0.9628 - val_loss: 0.0035 - val_accuracy: 0.9796\n",
            "Epoch 382/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0064 - accuracy: 0.9627 - val_loss: 0.0033 - val_accuracy: 0.9808\n",
            "Epoch 383/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0064 - accuracy: 0.9635 - val_loss: 0.0034 - val_accuracy: 0.9799\n",
            "Epoch 384/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0060 - accuracy: 0.9657 - val_loss: 0.0033 - val_accuracy: 0.9799\n",
            "Epoch 385/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0062 - accuracy: 0.9643 - val_loss: 0.0033 - val_accuracy: 0.9808\n",
            "Epoch 386/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0062 - accuracy: 0.9647 - val_loss: 0.0034 - val_accuracy: 0.9800\n",
            "Epoch 387/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0061 - accuracy: 0.9646 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 388/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0063 - accuracy: 0.9642 - val_loss: 0.0034 - val_accuracy: 0.9800\n",
            "Epoch 389/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0060 - accuracy: 0.9647 - val_loss: 0.0037 - val_accuracy: 0.9782\n",
            "Epoch 390/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0061 - accuracy: 0.9648 - val_loss: 0.0035 - val_accuracy: 0.9790\n",
            "Epoch 391/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0059 - accuracy: 0.9656 - val_loss: 0.0034 - val_accuracy: 0.9790\n",
            "Epoch 392/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0058 - accuracy: 0.9668 - val_loss: 0.0035 - val_accuracy: 0.9787\n",
            "Epoch 393/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0059 - accuracy: 0.9659 - val_loss: 0.0035 - val_accuracy: 0.9791\n",
            "Epoch 394/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0060 - accuracy: 0.9658 - val_loss: 0.0035 - val_accuracy: 0.9789\n",
            "Epoch 395/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0060 - accuracy: 0.9653 - val_loss: 0.0036 - val_accuracy: 0.9783\n",
            "Epoch 396/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0061 - accuracy: 0.9650 - val_loss: 0.0034 - val_accuracy: 0.9806\n",
            "Epoch 397/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0059 - accuracy: 0.9663 - val_loss: 0.0034 - val_accuracy: 0.9804\n",
            "Epoch 398/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0058 - accuracy: 0.9665 - val_loss: 0.0035 - val_accuracy: 0.9796\n",
            "Epoch 399/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0062 - accuracy: 0.9638 - val_loss: 0.0038 - val_accuracy: 0.9775\n",
            "Epoch 400/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0062 - accuracy: 0.9642 - val_loss: 0.0037 - val_accuracy: 0.9791\n",
            "Epoch 401/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0063 - accuracy: 0.9634 - val_loss: 0.0036 - val_accuracy: 0.9777\n",
            "Epoch 402/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0060 - accuracy: 0.9646 - val_loss: 0.0036 - val_accuracy: 0.9794\n",
            "Epoch 403/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0059 - accuracy: 0.9660 - val_loss: 0.0037 - val_accuracy: 0.9779\n",
            "Epoch 404/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0056 - accuracy: 0.9676 - val_loss: 0.0034 - val_accuracy: 0.9798\n",
            "Epoch 405/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0058 - accuracy: 0.9669 - val_loss: 0.0035 - val_accuracy: 0.9787\n",
            "Epoch 406/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0059 - accuracy: 0.9659 - val_loss: 0.0034 - val_accuracy: 0.9791\n",
            "Epoch 407/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0059 - accuracy: 0.9658 - val_loss: 0.0034 - val_accuracy: 0.9799\n",
            "Epoch 408/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0057 - accuracy: 0.9674 - val_loss: 0.0033 - val_accuracy: 0.9804\n",
            "Epoch 409/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0059 - accuracy: 0.9665 - val_loss: 0.0033 - val_accuracy: 0.9807\n",
            "Epoch 410/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0056 - accuracy: 0.9676 - val_loss: 0.0036 - val_accuracy: 0.9795\n",
            "Epoch 411/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0057 - accuracy: 0.9669 - val_loss: 0.0036 - val_accuracy: 0.9796\n",
            "Epoch 412/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0061 - accuracy: 0.9649 - val_loss: 0.0038 - val_accuracy: 0.9781\n",
            "Epoch 413/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0056 - accuracy: 0.9678 - val_loss: 0.0035 - val_accuracy: 0.9795\n",
            "Epoch 414/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0056 - accuracy: 0.9679 - val_loss: 0.0033 - val_accuracy: 0.9811\n",
            "Epoch 415/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0058 - accuracy: 0.9666 - val_loss: 0.0035 - val_accuracy: 0.9803\n",
            "Epoch 416/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0055 - accuracy: 0.9682 - val_loss: 0.0035 - val_accuracy: 0.9792\n",
            "Epoch 417/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0056 - accuracy: 0.9677 - val_loss: 0.0032 - val_accuracy: 0.9811\n",
            "Epoch 418/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9692 - val_loss: 0.0032 - val_accuracy: 0.9812\n",
            "Epoch 419/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0055 - accuracy: 0.9683 - val_loss: 0.0032 - val_accuracy: 0.9811\n",
            "Epoch 420/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9685 - val_loss: 0.0037 - val_accuracy: 0.9783\n",
            "Epoch 421/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0055 - accuracy: 0.9679 - val_loss: 0.0036 - val_accuracy: 0.9782\n",
            "Epoch 422/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0056 - accuracy: 0.9681 - val_loss: 0.0036 - val_accuracy: 0.9796\n",
            "Epoch 423/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9678 - val_loss: 0.0037 - val_accuracy: 0.9773\n",
            "Epoch 424/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0055 - accuracy: 0.9683 - val_loss: 0.0037 - val_accuracy: 0.9782\n",
            "Epoch 425/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0057 - accuracy: 0.9675 - val_loss: 0.0037 - val_accuracy: 0.9781\n",
            "Epoch 426/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0055 - accuracy: 0.9679 - val_loss: 0.0036 - val_accuracy: 0.9785\n",
            "Epoch 427/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0054 - accuracy: 0.9691 - val_loss: 0.0034 - val_accuracy: 0.9802\n",
            "Epoch 428/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0056 - accuracy: 0.9682 - val_loss: 0.0036 - val_accuracy: 0.9791\n",
            "Epoch 429/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0058 - accuracy: 0.9672 - val_loss: 0.0035 - val_accuracy: 0.9796\n",
            "Epoch 430/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0053 - accuracy: 0.9696 - val_loss: 0.0037 - val_accuracy: 0.9782\n",
            "Epoch 431/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9690 - val_loss: 0.0031 - val_accuracy: 0.9817\n",
            "Epoch 432/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0053 - accuracy: 0.9702 - val_loss: 0.0033 - val_accuracy: 0.9799\n",
            "Epoch 433/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0056 - accuracy: 0.9672 - val_loss: 0.0034 - val_accuracy: 0.9800\n",
            "Epoch 434/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9688 - val_loss: 0.0031 - val_accuracy: 0.9829\n",
            "Epoch 435/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9709 - val_loss: 0.0034 - val_accuracy: 0.9802\n",
            "Epoch 436/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9682 - val_loss: 0.0034 - val_accuracy: 0.9806\n",
            "Epoch 437/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0052 - accuracy: 0.9703 - val_loss: 0.0034 - val_accuracy: 0.9804\n",
            "Epoch 438/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9691 - val_loss: 0.0033 - val_accuracy: 0.9808\n",
            "Epoch 439/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9705 - val_loss: 0.0035 - val_accuracy: 0.9796\n",
            "Epoch 440/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0053 - accuracy: 0.9695 - val_loss: 0.0032 - val_accuracy: 0.9813\n",
            "Epoch 441/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0053 - accuracy: 0.9694 - val_loss: 0.0032 - val_accuracy: 0.9810\n",
            "Epoch 442/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0054 - accuracy: 0.9694 - val_loss: 0.0033 - val_accuracy: 0.9808\n",
            "Epoch 443/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0055 - accuracy: 0.9679 - val_loss: 0.0032 - val_accuracy: 0.9816\n",
            "Epoch 444/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0055 - accuracy: 0.9686 - val_loss: 0.0033 - val_accuracy: 0.9792\n",
            "Epoch 445/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0053 - accuracy: 0.9690 - val_loss: 0.0030 - val_accuracy: 0.9817\n",
            "Epoch 446/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9706 - val_loss: 0.0028 - val_accuracy: 0.9848\n",
            "Epoch 447/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0052 - accuracy: 0.9699 - val_loss: 0.0033 - val_accuracy: 0.9804\n",
            "Epoch 448/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0056 - accuracy: 0.9678 - val_loss: 0.0031 - val_accuracy: 0.9817\n",
            "Epoch 449/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0052 - accuracy: 0.9699 - val_loss: 0.0032 - val_accuracy: 0.9815\n",
            "Epoch 450/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0053 - accuracy: 0.9689 - val_loss: 0.0032 - val_accuracy: 0.9804\n",
            "Epoch 451/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9709 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 452/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9699 - val_loss: 0.0032 - val_accuracy: 0.9808\n",
            "Epoch 453/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9705 - val_loss: 0.0033 - val_accuracy: 0.9807\n",
            "Epoch 454/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9715 - val_loss: 0.0031 - val_accuracy: 0.9811\n",
            "Epoch 455/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9715 - val_loss: 0.0031 - val_accuracy: 0.9819\n",
            "Epoch 456/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9703 - val_loss: 0.0031 - val_accuracy: 0.9815\n",
            "Epoch 457/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0053 - accuracy: 0.9699 - val_loss: 0.0032 - val_accuracy: 0.9816\n",
            "Epoch 458/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0052 - accuracy: 0.9702 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 459/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9706 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 460/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9706 - val_loss: 0.0033 - val_accuracy: 0.9800\n",
            "Epoch 461/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0051 - accuracy: 0.9709 - val_loss: 0.0031 - val_accuracy: 0.9820\n",
            "Epoch 462/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0050 - accuracy: 0.9708 - val_loss: 0.0031 - val_accuracy: 0.9819\n",
            "Epoch 463/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0051 - accuracy: 0.9706 - val_loss: 0.0030 - val_accuracy: 0.9833\n",
            "Epoch 464/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9718 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 465/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9706 - val_loss: 0.0033 - val_accuracy: 0.9800\n",
            "Epoch 466/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9709 - val_loss: 0.0031 - val_accuracy: 0.9817\n",
            "Epoch 467/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9706 - val_loss: 0.0032 - val_accuracy: 0.9816\n",
            "Epoch 468/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9717 - val_loss: 0.0033 - val_accuracy: 0.9812\n",
            "Epoch 469/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9728 - val_loss: 0.0034 - val_accuracy: 0.9800\n",
            "Epoch 470/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0051 - accuracy: 0.9702 - val_loss: 0.0033 - val_accuracy: 0.9807\n",
            "Epoch 471/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0049 - accuracy: 0.9714 - val_loss: 0.0034 - val_accuracy: 0.9798\n",
            "Epoch 472/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0051 - accuracy: 0.9709 - val_loss: 0.0034 - val_accuracy: 0.9815\n",
            "Epoch 473/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9720 - val_loss: 0.0033 - val_accuracy: 0.9808\n",
            "Epoch 474/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0051 - accuracy: 0.9707 - val_loss: 0.0034 - val_accuracy: 0.9808\n",
            "Epoch 475/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9719 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 476/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9710 - val_loss: 0.0032 - val_accuracy: 0.9817\n",
            "Epoch 477/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9718 - val_loss: 0.0032 - val_accuracy: 0.9813\n",
            "Epoch 478/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9720 - val_loss: 0.0033 - val_accuracy: 0.9810\n",
            "Epoch 479/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9722 - val_loss: 0.0033 - val_accuracy: 0.9812\n",
            "Epoch 480/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0048 - accuracy: 0.9723 - val_loss: 0.0035 - val_accuracy: 0.9800\n",
            "Epoch 481/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9719 - val_loss: 0.0037 - val_accuracy: 0.9791\n",
            "Epoch 482/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0048 - accuracy: 0.9728 - val_loss: 0.0034 - val_accuracy: 0.9806\n",
            "Epoch 483/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0053 - accuracy: 0.9689 - val_loss: 0.0033 - val_accuracy: 0.9810\n",
            "Epoch 484/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9716 - val_loss: 0.0032 - val_accuracy: 0.9823\n",
            "Epoch 485/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0048 - accuracy: 0.9723 - val_loss: 0.0032 - val_accuracy: 0.9813\n",
            "Epoch 486/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0048 - accuracy: 0.9722 - val_loss: 0.0032 - val_accuracy: 0.9817\n",
            "Epoch 487/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9716 - val_loss: 0.0031 - val_accuracy: 0.9824\n",
            "Epoch 488/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0048 - accuracy: 0.9725 - val_loss: 0.0031 - val_accuracy: 0.9808\n",
            "Epoch 489/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0050 - accuracy: 0.9714 - val_loss: 0.0031 - val_accuracy: 0.9820\n",
            "Epoch 490/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9718 - val_loss: 0.0032 - val_accuracy: 0.9817\n",
            "Epoch 491/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0050 - accuracy: 0.9715 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 492/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0047 - accuracy: 0.9738 - val_loss: 0.0031 - val_accuracy: 0.9819\n",
            "Epoch 493/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0049 - accuracy: 0.9724 - val_loss: 0.0032 - val_accuracy: 0.9815\n",
            "Epoch 494/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0047 - accuracy: 0.9735 - val_loss: 0.0031 - val_accuracy: 0.9819\n",
            "Epoch 495/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0046 - accuracy: 0.9732 - val_loss: 0.0031 - val_accuracy: 0.9817\n",
            "Epoch 496/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0046 - accuracy: 0.9738 - val_loss: 0.0033 - val_accuracy: 0.9804\n",
            "Epoch 497/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0048 - accuracy: 0.9726 - val_loss: 0.0031 - val_accuracy: 0.9827\n",
            "Epoch 498/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0045 - accuracy: 0.9737 - val_loss: 0.0032 - val_accuracy: 0.9810\n",
            "Epoch 499/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0045 - accuracy: 0.9747 - val_loss: 0.0030 - val_accuracy: 0.9829\n",
            "Epoch 500/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0047 - accuracy: 0.9730 - val_loss: 0.0035 - val_accuracy: 0.9795\n",
            "Epoch 501/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0046 - accuracy: 0.9742 - val_loss: 0.0033 - val_accuracy: 0.9806\n",
            "Epoch 502/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0049 - accuracy: 0.9719 - val_loss: 0.0030 - val_accuracy: 0.9824\n",
            "Epoch 503/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0047 - accuracy: 0.9724 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 504/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0048 - accuracy: 0.9720 - val_loss: 0.0032 - val_accuracy: 0.9821\n",
            "Epoch 505/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0045 - accuracy: 0.9738 - val_loss: 0.0030 - val_accuracy: 0.9819\n",
            "Epoch 506/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0046 - accuracy: 0.9738 - val_loss: 0.0032 - val_accuracy: 0.9815\n",
            "Epoch 507/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0048 - accuracy: 0.9726 - val_loss: 0.0034 - val_accuracy: 0.9800\n",
            "Epoch 508/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0046 - accuracy: 0.9740 - val_loss: 0.0030 - val_accuracy: 0.9827\n",
            "Epoch 509/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0044 - accuracy: 0.9750 - val_loss: 0.0030 - val_accuracy: 0.9829\n",
            "Epoch 510/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0045 - accuracy: 0.9745 - val_loss: 0.0033 - val_accuracy: 0.9804\n",
            "Epoch 511/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0047 - accuracy: 0.9733 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 512/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0047 - accuracy: 0.9727 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 513/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0045 - accuracy: 0.9746 - val_loss: 0.0034 - val_accuracy: 0.9813\n",
            "Epoch 514/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0046 - accuracy: 0.9736 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 515/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0046 - accuracy: 0.9735 - val_loss: 0.0029 - val_accuracy: 0.9824\n",
            "Epoch 516/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0041 - accuracy: 0.9769 - val_loss: 0.0031 - val_accuracy: 0.9828\n",
            "Epoch 517/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0046 - accuracy: 0.9737 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 518/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0044 - accuracy: 0.9751 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 519/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0045 - accuracy: 0.9744 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 520/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9748 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 521/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0046 - accuracy: 0.9732 - val_loss: 0.0033 - val_accuracy: 0.9810\n",
            "Epoch 522/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0044 - accuracy: 0.9747 - val_loss: 0.0032 - val_accuracy: 0.9812\n",
            "Epoch 523/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0045 - accuracy: 0.9738 - val_loss: 0.0031 - val_accuracy: 0.9816\n",
            "Epoch 524/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0044 - accuracy: 0.9745 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 525/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0043 - accuracy: 0.9756 - val_loss: 0.0032 - val_accuracy: 0.9816\n",
            "Epoch 526/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0047 - accuracy: 0.9732 - val_loss: 0.0031 - val_accuracy: 0.9820\n",
            "Epoch 527/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0046 - accuracy: 0.9739 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 528/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0044 - accuracy: 0.9746 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 529/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0046 - accuracy: 0.9736 - val_loss: 0.0033 - val_accuracy: 0.9811\n",
            "Epoch 530/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0045 - accuracy: 0.9744 - val_loss: 0.0031 - val_accuracy: 0.9821\n",
            "Epoch 531/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0044 - accuracy: 0.9750 - val_loss: 0.0028 - val_accuracy: 0.9837\n",
            "Epoch 532/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0046 - accuracy: 0.9733 - val_loss: 0.0031 - val_accuracy: 0.9824\n",
            "Epoch 533/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0044 - accuracy: 0.9745 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 534/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0047 - accuracy: 0.9731 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 535/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0044 - accuracy: 0.9748 - val_loss: 0.0033 - val_accuracy: 0.9812\n",
            "Epoch 536/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0042 - accuracy: 0.9762 - val_loss: 0.0031 - val_accuracy: 0.9821\n",
            "Epoch 537/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0044 - accuracy: 0.9745 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 538/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0045 - accuracy: 0.9733 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 539/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0048 - accuracy: 0.9721 - val_loss: 0.0032 - val_accuracy: 0.9813\n",
            "Epoch 540/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0042 - accuracy: 0.9762 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 541/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0044 - accuracy: 0.9748 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 542/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0043 - accuracy: 0.9756 - val_loss: 0.0032 - val_accuracy: 0.9810\n",
            "Epoch 543/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9751 - val_loss: 0.0031 - val_accuracy: 0.9816\n",
            "Epoch 544/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9753 - val_loss: 0.0033 - val_accuracy: 0.9811\n",
            "Epoch 545/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0045 - accuracy: 0.9742 - val_loss: 0.0031 - val_accuracy: 0.9827\n",
            "Epoch 546/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0042 - accuracy: 0.9757 - val_loss: 0.0032 - val_accuracy: 0.9817\n",
            "Epoch 547/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0042 - accuracy: 0.9766 - val_loss: 0.0032 - val_accuracy: 0.9820\n",
            "Epoch 548/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9760 - val_loss: 0.0032 - val_accuracy: 0.9813\n",
            "Epoch 549/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9754 - val_loss: 0.0029 - val_accuracy: 0.9825\n",
            "Epoch 550/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0041 - accuracy: 0.9765 - val_loss: 0.0030 - val_accuracy: 0.9829\n",
            "Epoch 551/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0042 - accuracy: 0.9756 - val_loss: 0.0031 - val_accuracy: 0.9821\n",
            "Epoch 552/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9753 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 553/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0041 - accuracy: 0.9766 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 554/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0045 - accuracy: 0.9747 - val_loss: 0.0030 - val_accuracy: 0.9827\n",
            "Epoch 555/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0043 - accuracy: 0.9756 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 556/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0042 - accuracy: 0.9760 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 557/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0044 - accuracy: 0.9749 - val_loss: 0.0032 - val_accuracy: 0.9816\n",
            "Epoch 558/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0042 - accuracy: 0.9762 - val_loss: 0.0031 - val_accuracy: 0.9827\n",
            "Epoch 559/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9755 - val_loss: 0.0030 - val_accuracy: 0.9823\n",
            "Epoch 560/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9747 - val_loss: 0.0031 - val_accuracy: 0.9819\n",
            "Epoch 561/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0042 - accuracy: 0.9758 - val_loss: 0.0030 - val_accuracy: 0.9821\n",
            "Epoch 562/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9770 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 563/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0042 - accuracy: 0.9759 - val_loss: 0.0030 - val_accuracy: 0.9825\n",
            "Epoch 564/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0043 - accuracy: 0.9752 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 565/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0039 - accuracy: 0.9775 - val_loss: 0.0028 - val_accuracy: 0.9845\n",
            "Epoch 566/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0044 - accuracy: 0.9745 - val_loss: 0.0030 - val_accuracy: 0.9825\n",
            "Epoch 567/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0040 - accuracy: 0.9772 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 568/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0042 - accuracy: 0.9757 - val_loss: 0.0029 - val_accuracy: 0.9831\n",
            "Epoch 569/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0041 - accuracy: 0.9765 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 570/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0042 - accuracy: 0.9762 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 571/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0043 - accuracy: 0.9753 - val_loss: 0.0030 - val_accuracy: 0.9827\n",
            "Epoch 572/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0042 - accuracy: 0.9757 - val_loss: 0.0028 - val_accuracy: 0.9829\n",
            "Epoch 573/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0040 - accuracy: 0.9771 - val_loss: 0.0030 - val_accuracy: 0.9827\n",
            "Epoch 574/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0041 - accuracy: 0.9767 - val_loss: 0.0029 - val_accuracy: 0.9827\n",
            "Epoch 575/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0042 - accuracy: 0.9761 - val_loss: 0.0031 - val_accuracy: 0.9820\n",
            "Epoch 576/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0042 - accuracy: 0.9762 - val_loss: 0.0032 - val_accuracy: 0.9812\n",
            "Epoch 577/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0040 - accuracy: 0.9768 - val_loss: 0.0029 - val_accuracy: 0.9831\n",
            "Epoch 578/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0042 - accuracy: 0.9762 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 579/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0039 - accuracy: 0.9772 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 580/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0041 - accuracy: 0.9759 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 581/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0041 - accuracy: 0.9766 - val_loss: 0.0027 - val_accuracy: 0.9846\n",
            "Epoch 582/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0040 - accuracy: 0.9772 - val_loss: 0.0031 - val_accuracy: 0.9820\n",
            "Epoch 583/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0041 - accuracy: 0.9761 - val_loss: 0.0028 - val_accuracy: 0.9836\n",
            "Epoch 584/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0041 - accuracy: 0.9767 - val_loss: 0.0029 - val_accuracy: 0.9837\n",
            "Epoch 585/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0040 - accuracy: 0.9772 - val_loss: 0.0031 - val_accuracy: 0.9820\n",
            "Epoch 586/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0039 - accuracy: 0.9776 - val_loss: 0.0031 - val_accuracy: 0.9827\n",
            "Epoch 587/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0040 - accuracy: 0.9775 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 588/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0041 - accuracy: 0.9765 - val_loss: 0.0028 - val_accuracy: 0.9834\n",
            "Epoch 589/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0040 - accuracy: 0.9767 - val_loss: 0.0031 - val_accuracy: 0.9828\n",
            "Epoch 590/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0039 - accuracy: 0.9775 - val_loss: 0.0032 - val_accuracy: 0.9819\n",
            "Epoch 591/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9776 - val_loss: 0.0032 - val_accuracy: 0.9817\n",
            "Epoch 592/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0043 - accuracy: 0.9753 - val_loss: 0.0031 - val_accuracy: 0.9825\n",
            "Epoch 593/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9788 - val_loss: 0.0033 - val_accuracy: 0.9815\n",
            "Epoch 594/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0038 - accuracy: 0.9783 - val_loss: 0.0030 - val_accuracy: 0.9823\n",
            "Epoch 595/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0039 - accuracy: 0.9782 - val_loss: 0.0032 - val_accuracy: 0.9817\n",
            "Epoch 596/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0039 - accuracy: 0.9778 - val_loss: 0.0032 - val_accuracy: 0.9819\n",
            "Epoch 597/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0039 - accuracy: 0.9775 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 598/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0040 - accuracy: 0.9768 - val_loss: 0.0030 - val_accuracy: 0.9825\n",
            "Epoch 599/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9767 - val_loss: 0.0032 - val_accuracy: 0.9823\n",
            "Epoch 600/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0040 - accuracy: 0.9773 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 601/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0042 - accuracy: 0.9764 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 602/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0039 - accuracy: 0.9774 - val_loss: 0.0031 - val_accuracy: 0.9819\n",
            "Epoch 603/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0044 - accuracy: 0.9751 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 604/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0039 - accuracy: 0.9779 - val_loss: 0.0030 - val_accuracy: 0.9837\n",
            "Epoch 605/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0040 - accuracy: 0.9772 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 606/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9773 - val_loss: 0.0028 - val_accuracy: 0.9836\n",
            "Epoch 607/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0039 - accuracy: 0.9781 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 608/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0038 - accuracy: 0.9786 - val_loss: 0.0031 - val_accuracy: 0.9817\n",
            "Epoch 609/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0040 - accuracy: 0.9771 - val_loss: 0.0032 - val_accuracy: 0.9827\n",
            "Epoch 610/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9792 - val_loss: 0.0030 - val_accuracy: 0.9824\n",
            "Epoch 611/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0037 - accuracy: 0.9789 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 612/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0040 - accuracy: 0.9772 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 613/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9789 - val_loss: 0.0030 - val_accuracy: 0.9841\n",
            "Epoch 614/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0039 - accuracy: 0.9777 - val_loss: 0.0030 - val_accuracy: 0.9832\n",
            "Epoch 615/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0036 - accuracy: 0.9794 - val_loss: 0.0032 - val_accuracy: 0.9823\n",
            "Epoch 616/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9787 - val_loss: 0.0030 - val_accuracy: 0.9824\n",
            "Epoch 617/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9791 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 618/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9774 - val_loss: 0.0029 - val_accuracy: 0.9831\n",
            "Epoch 619/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9770 - val_loss: 0.0031 - val_accuracy: 0.9828\n",
            "Epoch 620/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0039 - accuracy: 0.9775 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 621/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9776 - val_loss: 0.0031 - val_accuracy: 0.9824\n",
            "Epoch 622/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0038 - accuracy: 0.9781 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 623/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0038 - accuracy: 0.9785 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 624/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0038 - accuracy: 0.9782 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 625/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9792 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 626/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0038 - accuracy: 0.9782 - val_loss: 0.0027 - val_accuracy: 0.9849\n",
            "Epoch 627/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0039 - accuracy: 0.9781 - val_loss: 0.0027 - val_accuracy: 0.9841\n",
            "Epoch 628/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0040 - accuracy: 0.9768 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 629/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9791 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 630/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0036 - accuracy: 0.9794 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 631/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0036 - accuracy: 0.9799 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 632/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9790 - val_loss: 0.0031 - val_accuracy: 0.9828\n",
            "Epoch 633/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9803 - val_loss: 0.0029 - val_accuracy: 0.9837\n",
            "Epoch 634/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9788 - val_loss: 0.0028 - val_accuracy: 0.9846\n",
            "Epoch 635/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0036 - accuracy: 0.9794 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 636/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9792 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 637/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0039 - accuracy: 0.9775 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 638/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0038 - accuracy: 0.9781 - val_loss: 0.0029 - val_accuracy: 0.9827\n",
            "Epoch 639/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0038 - accuracy: 0.9784 - val_loss: 0.0027 - val_accuracy: 0.9853\n",
            "Epoch 640/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9791 - val_loss: 0.0029 - val_accuracy: 0.9842\n",
            "Epoch 641/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9791 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 642/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9796 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 643/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0038 - accuracy: 0.9789 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 644/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9794 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 645/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0034 - accuracy: 0.9810 - val_loss: 0.0028 - val_accuracy: 0.9845\n",
            "Epoch 646/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0035 - accuracy: 0.9797 - val_loss: 0.0026 - val_accuracy: 0.9852\n",
            "Epoch 647/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0038 - accuracy: 0.9787 - val_loss: 0.0029 - val_accuracy: 0.9829\n",
            "Epoch 648/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9796 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 649/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9787 - val_loss: 0.0032 - val_accuracy: 0.9821\n",
            "Epoch 650/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0038 - accuracy: 0.9786 - val_loss: 0.0030 - val_accuracy: 0.9833\n",
            "Epoch 651/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9802 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 652/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9788 - val_loss: 0.0028 - val_accuracy: 0.9837\n",
            "Epoch 653/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0038 - accuracy: 0.9786 - val_loss: 0.0027 - val_accuracy: 0.9845\n",
            "Epoch 654/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0037 - accuracy: 0.9787 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 655/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9795 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 656/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0036 - accuracy: 0.9799 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 657/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0038 - accuracy: 0.9779 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 658/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9791 - val_loss: 0.0030 - val_accuracy: 0.9833\n",
            "Epoch 659/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9795 - val_loss: 0.0034 - val_accuracy: 0.9804\n",
            "Epoch 660/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9796 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 661/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0034 - accuracy: 0.9804 - val_loss: 0.0029 - val_accuracy: 0.9829\n",
            "Epoch 662/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9786 - val_loss: 0.0032 - val_accuracy: 0.9815\n",
            "Epoch 663/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9785 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 664/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0040 - accuracy: 0.9765 - val_loss: 0.0027 - val_accuracy: 0.9841\n",
            "Epoch 665/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0036 - accuracy: 0.9796 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 666/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9791 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 667/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9800 - val_loss: 0.0026 - val_accuracy: 0.9856\n",
            "Epoch 668/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0037 - accuracy: 0.9791 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 669/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9788 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 670/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9794 - val_loss: 0.0030 - val_accuracy: 0.9836\n",
            "Epoch 671/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9787 - val_loss: 0.0031 - val_accuracy: 0.9825\n",
            "Epoch 672/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0035 - accuracy: 0.9804 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 673/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0036 - accuracy: 0.9795 - val_loss: 0.0031 - val_accuracy: 0.9827\n",
            "Epoch 674/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0037 - accuracy: 0.9786 - val_loss: 0.0030 - val_accuracy: 0.9825\n",
            "Epoch 675/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0036 - accuracy: 0.9798 - val_loss: 0.0030 - val_accuracy: 0.9831\n",
            "Epoch 676/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0034 - accuracy: 0.9802 - val_loss: 0.0028 - val_accuracy: 0.9837\n",
            "Epoch 677/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0032 - accuracy: 0.9818 - val_loss: 0.0027 - val_accuracy: 0.9850\n",
            "Epoch 678/900\n",
            "204/204 [==============================] - 5s 27ms/step - loss: 0.0036 - accuracy: 0.9792 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 679/900\n",
            "204/204 [==============================] - 5s 26ms/step - loss: 0.0035 - accuracy: 0.9803 - val_loss: 0.0027 - val_accuracy: 0.9850\n",
            "Epoch 680/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0036 - accuracy: 0.9793 - val_loss: 0.0026 - val_accuracy: 0.9848\n",
            "Epoch 681/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9793 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 682/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0037 - accuracy: 0.9792 - val_loss: 0.0031 - val_accuracy: 0.9824\n",
            "Epoch 683/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9802 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 684/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0035 - accuracy: 0.9800 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 685/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0035 - accuracy: 0.9797 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 686/900\n",
            "204/204 [==============================] - 6s 30ms/step - loss: 0.0035 - accuracy: 0.9802 - val_loss: 0.0026 - val_accuracy: 0.9858\n",
            "Epoch 687/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0036 - accuracy: 0.9798 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 688/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0033 - accuracy: 0.9813 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 689/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0037 - accuracy: 0.9784 - val_loss: 0.0027 - val_accuracy: 0.9845\n",
            "Epoch 690/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0035 - accuracy: 0.9804 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 691/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9794 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 692/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9803 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 693/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9797 - val_loss: 0.0025 - val_accuracy: 0.9856\n",
            "Epoch 694/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0030 - val_accuracy: 0.9842\n",
            "Epoch 695/900\n",
            "204/204 [==============================] - 6s 30ms/step - loss: 0.0038 - accuracy: 0.9783 - val_loss: 0.0030 - val_accuracy: 0.9834\n",
            "Epoch 696/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0028 - val_accuracy: 0.9845\n",
            "Epoch 697/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0037 - accuracy: 0.9788 - val_loss: 0.0025 - val_accuracy: 0.9863\n",
            "Epoch 698/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0035 - accuracy: 0.9794 - val_loss: 0.0028 - val_accuracy: 0.9845\n",
            "Epoch 699/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0033 - accuracy: 0.9810 - val_loss: 0.0028 - val_accuracy: 0.9848\n",
            "Epoch 700/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0033 - accuracy: 0.9805 - val_loss: 0.0026 - val_accuracy: 0.9857\n",
            "Epoch 701/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0036 - accuracy: 0.9795 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 702/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9793 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 703/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0034 - accuracy: 0.9806 - val_loss: 0.0030 - val_accuracy: 0.9834\n",
            "Epoch 704/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0034 - accuracy: 0.9807 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 705/900\n",
            "204/204 [==============================] - 6s 29ms/step - loss: 0.0032 - accuracy: 0.9818 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 706/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0035 - accuracy: 0.9797 - val_loss: 0.0027 - val_accuracy: 0.9837\n",
            "Epoch 707/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0036 - accuracy: 0.9798 - val_loss: 0.0026 - val_accuracy: 0.9856\n",
            "Epoch 708/900\n",
            "204/204 [==============================] - 6s 28ms/step - loss: 0.0034 - accuracy: 0.9808 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 709/900\n",
            "204/204 [==============================] - 6s 27ms/step - loss: 0.0035 - accuracy: 0.9800 - val_loss: 0.0031 - val_accuracy: 0.9821\n",
            "Epoch 710/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0035 - accuracy: 0.9799 - val_loss: 0.0029 - val_accuracy: 0.9837\n",
            "Epoch 711/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0036 - accuracy: 0.9796 - val_loss: 0.0027 - val_accuracy: 0.9849\n",
            "Epoch 712/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0036 - accuracy: 0.9793 - val_loss: 0.0026 - val_accuracy: 0.9853\n",
            "Epoch 713/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9809 - val_loss: 0.0030 - val_accuracy: 0.9825\n",
            "Epoch 714/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9806 - val_loss: 0.0027 - val_accuracy: 0.9846\n",
            "Epoch 715/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9806 - val_loss: 0.0025 - val_accuracy: 0.9856\n",
            "Epoch 716/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9808 - val_loss: 0.0027 - val_accuracy: 0.9853\n",
            "Epoch 717/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9815 - val_loss: 0.0030 - val_accuracy: 0.9829\n",
            "Epoch 718/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0034 - accuracy: 0.9810 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 719/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0035 - accuracy: 0.9806 - val_loss: 0.0026 - val_accuracy: 0.9858\n",
            "Epoch 720/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0035 - accuracy: 0.9802 - val_loss: 0.0026 - val_accuracy: 0.9850\n",
            "Epoch 721/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0032 - accuracy: 0.9819 - val_loss: 0.0032 - val_accuracy: 0.9819\n",
            "Epoch 722/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0035 - accuracy: 0.9799 - val_loss: 0.0026 - val_accuracy: 0.9852\n",
            "Epoch 723/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0025 - val_accuracy: 0.9858\n",
            "Epoch 724/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9818 - val_loss: 0.0027 - val_accuracy: 0.9846\n",
            "Epoch 725/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0036 - accuracy: 0.9786 - val_loss: 0.0026 - val_accuracy: 0.9853\n",
            "Epoch 726/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9810 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 727/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9824 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 728/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0035 - accuracy: 0.9800 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 729/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9806 - val_loss: 0.0026 - val_accuracy: 0.9850\n",
            "Epoch 730/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0035 - accuracy: 0.9798 - val_loss: 0.0027 - val_accuracy: 0.9841\n",
            "Epoch 731/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9807 - val_loss: 0.0028 - val_accuracy: 0.9848\n",
            "Epoch 732/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0034 - accuracy: 0.9808 - val_loss: 0.0029 - val_accuracy: 0.9846\n",
            "Epoch 733/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9812 - val_loss: 0.0026 - val_accuracy: 0.9853\n",
            "Epoch 734/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9815 - val_loss: 0.0030 - val_accuracy: 0.9834\n",
            "Epoch 735/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 736/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0034 - accuracy: 0.9810 - val_loss: 0.0030 - val_accuracy: 0.9833\n",
            "Epoch 737/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9824 - val_loss: 0.0025 - val_accuracy: 0.9857\n",
            "Epoch 738/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9806 - val_loss: 0.0024 - val_accuracy: 0.9858\n",
            "Epoch 739/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9816 - val_loss: 0.0027 - val_accuracy: 0.9846\n",
            "Epoch 740/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0035 - accuracy: 0.9801 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 741/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0035 - accuracy: 0.9800 - val_loss: 0.0026 - val_accuracy: 0.9853\n",
            "Epoch 742/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9823 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 743/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9826 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 744/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0036 - accuracy: 0.9791 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 745/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9812 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 746/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 747/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9825 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 748/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9819 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 749/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9802 - val_loss: 0.0029 - val_accuracy: 0.9832\n",
            "Epoch 750/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9815 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 751/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0030 - val_accuracy: 0.9832\n",
            "Epoch 752/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0035 - accuracy: 0.9798 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 753/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9819 - val_loss: 0.0027 - val_accuracy: 0.9849\n",
            "Epoch 754/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0029 - val_accuracy: 0.9837\n",
            "Epoch 755/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9832 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 756/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9818 - val_loss: 0.0029 - val_accuracy: 0.9837\n",
            "Epoch 757/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9818 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 758/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9819 - val_loss: 0.0033 - val_accuracy: 0.9813\n",
            "Epoch 759/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9815 - val_loss: 0.0030 - val_accuracy: 0.9832\n",
            "Epoch 760/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9823 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 761/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9824 - val_loss: 0.0030 - val_accuracy: 0.9834\n",
            "Epoch 762/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9823 - val_loss: 0.0030 - val_accuracy: 0.9832\n",
            "Epoch 763/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0033 - accuracy: 0.9809 - val_loss: 0.0028 - val_accuracy: 0.9832\n",
            "Epoch 764/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0031 - val_accuracy: 0.9823\n",
            "Epoch 765/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9814 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 766/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9820 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 767/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9823 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 768/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9820 - val_loss: 0.0032 - val_accuracy: 0.9821\n",
            "Epoch 769/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 770/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9821 - val_loss: 0.0030 - val_accuracy: 0.9832\n",
            "Epoch 771/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9816 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 772/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9826 - val_loss: 0.0031 - val_accuracy: 0.9831\n",
            "Epoch 773/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9819 - val_loss: 0.0029 - val_accuracy: 0.9837\n",
            "Epoch 774/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9829 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 775/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9816 - val_loss: 0.0028 - val_accuracy: 0.9848\n",
            "Epoch 776/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9824 - val_loss: 0.0031 - val_accuracy: 0.9828\n",
            "Epoch 777/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0026 - val_accuracy: 0.9854\n",
            "Epoch 778/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9829 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 779/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0028 - accuracy: 0.9840 - val_loss: 0.0026 - val_accuracy: 0.9854\n",
            "Epoch 780/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9804 - val_loss: 0.0029 - val_accuracy: 0.9841\n",
            "Epoch 781/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9831 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 782/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9825 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 783/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9814 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 784/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9821 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 785/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9812 - val_loss: 0.0027 - val_accuracy: 0.9853\n",
            "Epoch 786/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9827 - val_loss: 0.0029 - val_accuracy: 0.9840\n",
            "Epoch 787/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0027 - val_accuracy: 0.9853\n",
            "Epoch 788/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9829 - val_loss: 0.0030 - val_accuracy: 0.9828\n",
            "Epoch 789/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9815 - val_loss: 0.0027 - val_accuracy: 0.9844\n",
            "Epoch 790/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9829 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 791/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9827 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 792/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9820 - val_loss: 0.0032 - val_accuracy: 0.9824\n",
            "Epoch 793/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0033 - val_accuracy: 0.9813\n",
            "Epoch 794/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9824 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 795/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9827 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 796/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0029 - accuracy: 0.9834 - val_loss: 0.0028 - val_accuracy: 0.9844\n",
            "Epoch 797/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9815 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 798/900\n",
            "204/204 [==============================] - 5s 22ms/step - loss: 0.0029 - accuracy: 0.9832 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 799/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9823 - val_loss: 0.0030 - val_accuracy: 0.9827\n",
            "Epoch 800/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0026 - val_accuracy: 0.9854\n",
            "Epoch 801/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0034 - accuracy: 0.9807 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 802/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0033 - accuracy: 0.9811 - val_loss: 0.0028 - val_accuracy: 0.9848\n",
            "Epoch 803/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9824 - val_loss: 0.0031 - val_accuracy: 0.9825\n",
            "Epoch 804/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9832 - val_loss: 0.0027 - val_accuracy: 0.9853\n",
            "Epoch 805/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9818 - val_loss: 0.0030 - val_accuracy: 0.9837\n",
            "Epoch 806/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9837 - val_loss: 0.0026 - val_accuracy: 0.9853\n",
            "Epoch 807/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9821 - val_loss: 0.0026 - val_accuracy: 0.9857\n",
            "Epoch 808/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0032 - accuracy: 0.9818 - val_loss: 0.0026 - val_accuracy: 0.9856\n",
            "Epoch 809/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0032 - accuracy: 0.9825 - val_loss: 0.0026 - val_accuracy: 0.9852\n",
            "Epoch 810/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9836 - val_loss: 0.0027 - val_accuracy: 0.9850\n",
            "Epoch 811/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9826 - val_loss: 0.0026 - val_accuracy: 0.9858\n",
            "Epoch 812/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9839 - val_loss: 0.0028 - val_accuracy: 0.9849\n",
            "Epoch 813/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9834 - val_loss: 0.0029 - val_accuracy: 0.9837\n",
            "Epoch 814/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9822 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 815/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9823 - val_loss: 0.0025 - val_accuracy: 0.9856\n",
            "Epoch 816/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0032 - accuracy: 0.9818 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 817/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9822 - val_loss: 0.0026 - val_accuracy: 0.9850\n",
            "Epoch 818/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9821 - val_loss: 0.0027 - val_accuracy: 0.9849\n",
            "Epoch 819/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9829 - val_loss: 0.0027 - val_accuracy: 0.9850\n",
            "Epoch 820/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9825 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 821/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9825 - val_loss: 0.0029 - val_accuracy: 0.9836\n",
            "Epoch 822/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9837 - val_loss: 0.0030 - val_accuracy: 0.9838\n",
            "Epoch 823/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0026 - val_accuracy: 0.9849\n",
            "Epoch 824/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0026 - val_accuracy: 0.9857\n",
            "Epoch 825/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9820 - val_loss: 0.0026 - val_accuracy: 0.9853\n",
            "Epoch 826/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9829 - val_loss: 0.0026 - val_accuracy: 0.9859\n",
            "Epoch 827/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9823 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 828/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9841 - val_loss: 0.0029 - val_accuracy: 0.9842\n",
            "Epoch 829/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9835 - val_loss: 0.0026 - val_accuracy: 0.9849\n",
            "Epoch 830/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9840 - val_loss: 0.0028 - val_accuracy: 0.9838\n",
            "Epoch 831/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0032 - accuracy: 0.9815 - val_loss: 0.0027 - val_accuracy: 0.9850\n",
            "Epoch 832/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9832 - val_loss: 0.0024 - val_accuracy: 0.9863\n",
            "Epoch 833/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0027 - val_accuracy: 0.9845\n",
            "Epoch 834/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9842 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 835/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0032 - accuracy: 0.9817 - val_loss: 0.0026 - val_accuracy: 0.9856\n",
            "Epoch 836/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9823 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 837/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9837 - val_loss: 0.0025 - val_accuracy: 0.9852\n",
            "Epoch 838/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9839 - val_loss: 0.0025 - val_accuracy: 0.9852\n",
            "Epoch 839/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9835 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 840/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9831 - val_loss: 0.0027 - val_accuracy: 0.9853\n",
            "Epoch 841/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9835 - val_loss: 0.0026 - val_accuracy: 0.9852\n",
            "Epoch 842/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9829 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 843/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0027 - val_accuracy: 0.9853\n",
            "Epoch 844/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9832 - val_loss: 0.0027 - val_accuracy: 0.9849\n",
            "Epoch 845/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9822 - val_loss: 0.0028 - val_accuracy: 0.9850\n",
            "Epoch 846/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9832 - val_loss: 0.0029 - val_accuracy: 0.9834\n",
            "Epoch 847/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9824 - val_loss: 0.0027 - val_accuracy: 0.9846\n",
            "Epoch 848/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9827 - val_loss: 0.0026 - val_accuracy: 0.9853\n",
            "Epoch 849/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9841 - val_loss: 0.0028 - val_accuracy: 0.9845\n",
            "Epoch 850/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0028 - val_accuracy: 0.9849\n",
            "Epoch 851/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9823 - val_loss: 0.0025 - val_accuracy: 0.9861\n",
            "Epoch 852/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9834 - val_loss: 0.0029 - val_accuracy: 0.9841\n",
            "Epoch 853/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0027 - val_accuracy: 0.9850\n",
            "Epoch 854/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9837 - val_loss: 0.0025 - val_accuracy: 0.9858\n",
            "Epoch 855/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0029 - accuracy: 0.9836 - val_loss: 0.0026 - val_accuracy: 0.9858\n",
            "Epoch 856/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0029 - accuracy: 0.9837 - val_loss: 0.0025 - val_accuracy: 0.9856\n",
            "Epoch 857/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9827 - val_loss: 0.0024 - val_accuracy: 0.9874\n",
            "Epoch 858/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9835 - val_loss: 0.0025 - val_accuracy: 0.9858\n",
            "Epoch 859/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9830 - val_loss: 0.0027 - val_accuracy: 0.9856\n",
            "Epoch 860/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9837 - val_loss: 0.0028 - val_accuracy: 0.9841\n",
            "Epoch 861/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0029 - val_accuracy: 0.9842\n",
            "Epoch 862/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0029 - accuracy: 0.9838 - val_loss: 0.0026 - val_accuracy: 0.9861\n",
            "Epoch 863/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0027 - accuracy: 0.9843 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 864/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9825 - val_loss: 0.0027 - val_accuracy: 0.9845\n",
            "Epoch 865/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9834 - val_loss: 0.0027 - val_accuracy: 0.9840\n",
            "Epoch 866/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9837 - val_loss: 0.0027 - val_accuracy: 0.9852\n",
            "Epoch 867/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0026 - accuracy: 0.9850 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 868/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9841 - val_loss: 0.0026 - val_accuracy: 0.9852\n",
            "Epoch 869/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0025 - val_accuracy: 0.9862\n",
            "Epoch 870/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0027 - accuracy: 0.9850 - val_loss: 0.0029 - val_accuracy: 0.9840\n",
            "Epoch 871/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9819 - val_loss: 0.0026 - val_accuracy: 0.9856\n",
            "Epoch 872/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9834 - val_loss: 0.0029 - val_accuracy: 0.9838\n",
            "Epoch 873/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0027 - accuracy: 0.9845 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 874/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9841 - val_loss: 0.0024 - val_accuracy: 0.9865\n",
            "Epoch 875/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9834 - val_loss: 0.0025 - val_accuracy: 0.9858\n",
            "Epoch 876/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9829 - val_loss: 0.0027 - val_accuracy: 0.9844\n",
            "Epoch 877/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9835 - val_loss: 0.0028 - val_accuracy: 0.9836\n",
            "Epoch 878/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9830 - val_loss: 0.0032 - val_accuracy: 0.9823\n",
            "Epoch 879/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9833 - val_loss: 0.0028 - val_accuracy: 0.9840\n",
            "Epoch 880/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0026 - accuracy: 0.9858 - val_loss: 0.0027 - val_accuracy: 0.9845\n",
            "Epoch 881/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9827 - val_loss: 0.0031 - val_accuracy: 0.9832\n",
            "Epoch 882/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9846 - val_loss: 0.0029 - val_accuracy: 0.9833\n",
            "Epoch 883/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9829 - val_loss: 0.0028 - val_accuracy: 0.9833\n",
            "Epoch 884/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9846 - val_loss: 0.0028 - val_accuracy: 0.9842\n",
            "Epoch 885/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0028 - accuracy: 0.9841 - val_loss: 0.0029 - val_accuracy: 0.9841\n",
            "Epoch 886/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0027 - accuracy: 0.9843 - val_loss: 0.0025 - val_accuracy: 0.9861\n",
            "Epoch 887/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0028 - accuracy: 0.9841 - val_loss: 0.0027 - val_accuracy: 0.9846\n",
            "Epoch 888/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0028 - accuracy: 0.9837 - val_loss: 0.0026 - val_accuracy: 0.9854\n",
            "Epoch 889/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0031 - accuracy: 0.9818 - val_loss: 0.0028 - val_accuracy: 0.9846\n",
            "Epoch 890/900\n",
            "204/204 [==============================] - 5s 25ms/step - loss: 0.0028 - accuracy: 0.9845 - val_loss: 0.0025 - val_accuracy: 0.9861\n",
            "Epoch 891/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9843 - val_loss: 0.0027 - val_accuracy: 0.9841\n",
            "Epoch 892/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9835 - val_loss: 0.0026 - val_accuracy: 0.9852\n",
            "Epoch 893/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0031 - accuracy: 0.9819 - val_loss: 0.0027 - val_accuracy: 0.9849\n",
            "Epoch 894/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0028 - accuracy: 0.9843 - val_loss: 0.0027 - val_accuracy: 0.9854\n",
            "Epoch 895/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9830 - val_loss: 0.0027 - val_accuracy: 0.9848\n",
            "Epoch 896/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0029 - accuracy: 0.9834 - val_loss: 0.0027 - val_accuracy: 0.9842\n",
            "Epoch 897/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0030 - accuracy: 0.9827 - val_loss: 0.0026 - val_accuracy: 0.9854\n",
            "Epoch 898/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0028 - accuracy: 0.9847 - val_loss: 0.0025 - val_accuracy: 0.9858\n",
            "Epoch 899/900\n",
            "204/204 [==============================] - 5s 23ms/step - loss: 0.0031 - accuracy: 0.9825 - val_loss: 0.0026 - val_accuracy: 0.9858\n",
            "Epoch 900/900\n",
            "204/204 [==============================] - 5s 24ms/step - loss: 0.0027 - accuracy: 0.9843 - val_loss: 0.0027 - val_accuracy: 0.9858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcBv7Omih4Mx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}